---
title: "Medical Insurance Cost Prediction"
author: 'SL group 6'
date: "2025-12-03"
header-includes:
  - \usepackage{ctex}
output:
  pdf_document: 
    latex_engine: xelatex
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Introduction

### 1.1 Problem Definition

### 1.2 About Dataset

This dataset provides information about 100,000 individuals including
their demographics, socioeconomic status, health conditions, lifestyle
factors, Insurance plans, and medical expenditures.

In detail, the columns are:

1.  Demographics & Socioeconomic: person_id, age, sex, region,
    urban_rural, income, education, marital_status, employment_status,
    household_size, dependents

2.  Lifestyle & Habits: bmi, smoker, alcohol_freq, exercise_frequency,
    sleep_hours, stress_level

3.  Health & Clinical: hypertension, diabetes, copd, cardiovascular,
    cancer_history, kidney_disease, liver_disease, arthritis,
    mental_health, chronic_count, systolic_bp, diastolic_bp, ldl, hba1c,
    risk_score, is_high_risk

4.  Healthcare Utilization & Procedures: visits_last_year,
    hospitalizations_last_3yrs, days_hospitalized_last_3yrs,
    medication_count, proc_imaging, proc_surgery, proc_psycho,
    proc_consult_count, proc_lab, had_major

5.  Insurance & Policy: plan_type, network_tier, deductible, copay,
    policy_term_years, policy_changes_last_2yrs, provider_quality

6.  Medical Costs & Claims: annual_medical_cost, annual_premium,
    monthly_premium, claims_count, avg_claim_amount, total_claims_paid

## 2. Exploratory Data Analysis (EDA)

-   Libraries

```{r,warning=FALSE,message=FALSE}
# ============================
# Load libraries
# ============================

#install.packages("caret")
library(glmnet)
library(ggplot2)
library(dplyr)
library(tidyr)
library(ggforce)
library(gridExtra)
library(reshape2)
library(GGally)
library(corrplot)
library(e1071)
library(rpart)
library(rpart.plot)
library(tree)
library(randomForest)
library(caret)
library(xgboost)
library(Matrix)
library(caTools)
library(leaps)
library(scales)
```

### 2.1. Load and Inspect the dataset

```{r}
# ============================
# Load and inspect the dataset
# ============================

# Load dataset
Insurance <- read.csv("medical_insurance.csv", stringsAsFactors = FALSE)

# Quick structure and preview
str(Insurance)          # Check variable types
head(Insurance)         # Preview first few rows
summary(Insurance)      # Summary statistics

# ============================
# Check and handle missing values
# ============================

# Count missing values in each column
na_counts <- sum(is.na(Insurance))
na_counts # 0 NA data

# Count how many rows have at least one missing value
missing_rows <- sum(!complete.cases(Insurance))
missing_rows # 0 missing data

```

### 2.2. Univariate Analysis

```{r}
# ============================
# Univariate Analysis
# ============================

Insurance_view <- Insurance

# convert 0/1 columns to factors
#discrete_cols <- c("hypertension", "diabetes", "asthma", "copd", "cardiovascular_disease", "cancer_history", "kidney_disease", "liver_disease", "arthritis", "mental_health", "is_high_risk", "had_major_procedure")
discrete_cols <- c()
#Insurance_view[discrete_cols] <- lapply(Insurance_view[discrete_cols], factor)


# Convert all character columns to factors
for (v in names(Insurance_view)) {
  if (is.character(Insurance_view[[v]])) {
    Insurance_view[[v]] <- factor(Insurance_view[[v]])
  }
}


num_cols <- names(Insurance_view)[sapply(Insurance_view, is.numeric)]
cat_cols <- names(Insurance_view)[sapply(Insurance_view, is.factor)]

plot_list <- list()

# ---- Numeric Variable: Histograms ----
for (v in num_cols) {
  p <- ggplot(Insurance_view, aes(x = .data[[v]])) + 
    geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "steelblue", alpha = 0.7) +
    theme(axis.text.x = element_text(size = 4)) +

    theme_minimal()

  plot_list[[v]] <- p
}

# ---- Category Variable: Barplots ----
for (v in cat_cols) {
  p <- ggplot(Insurance_view, aes(x = .data[[v]])) + 
    geom_bar(fill = "lightgreen", color = "black") +
    geom_text(stat='count', aes(label = after_stat(count)), vjust = -0.5, size = 2) +
    theme(axis.text.x = element_text(size = 6,angle = 45, hjust = 1)) +
    theme_minimal()
  
  plot_list[[v]] <- p
}

plot_row <- 2
plot_col <- 2
plot_total <- plot_col*plot_row
num_plot <- length(plot_list)
num_pages <- floor(num_plot / plot_total)

for (page in 1:num_pages) {
  beg <- (page-1) * plot_total + 1; end <- page * plot_total
  grid.arrange(grobs = plot_list[beg:end], 
               ncol = plot_col, nrow = plot_row)
  if (page == num_pages && num_plot %% plot_total > 0) {
    grid.arrange(grobs = plot_list[(end+1):num_plot], ncol = plot_col)
  }
}




```

```{r}
# Assuming your dataset is named 'Insurance'
Insurance_view <- Insurance

# ============================
# 0. Data Preprocessing (Preserving your logic)
# ============================
# Convert all character columns to factors
for (v in names(Insurance_view)) {
  if (is.character(Insurance_view[[v]])) {
    Insurance_view[[v]] <- factor(Insurance_view[[v]])
  }
}

# ============================
# 1. Q-Q Plots (For continuous variables)
# ============================

# 1.1 BMI Q-Q Plot
p_qq_bmi <- ggplot(Insurance_view, aes(sample = bmi)) +
  stat_qq(color = "steelblue", alpha = 0.5) +
  stat_qq_line(color = "red", lwd = 1) +
  labs(title = "Q-Q Plot: BMI", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_minimal()

# 1.2 Log(Charges) Q-Q Plot
# Note: Using natural log log() here; replace with log10 if needed
p_qq_charges <- ggplot(Insurance_view, aes(sample = log1p(charges))) +
  stat_qq(color = "steelblue", alpha = 0.5) +
  stat_qq_line(color = "red", lwd = 1) +
  labs(title = "Q-Q Plot: Log1p(Charges)", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_minimal()

# Display Q-Q Plots
grid.arrange(p_qq_bmi, p_qq_charges, ncol = 2)


# ============================
# 2. Pie Charts (For categorical variables)
# ============================

# Define a function to draw pie charts to avoid repeating code
draw_pie_chart <- function(data, col_name) {
  # 1. Calculate counts and percentages
  plot_data <- data %>%
    count(.data[[col_name]]) %>%
    mutate(
      perc = n / sum(n),
      labels = scales::percent(perc) # Convert to percentage format (e.g., 50%)
    ) %>%
    arrange(desc(perc)) # Sort to make the chart look better

  # 2. Plotting
  ggplot(plot_data, aes(x = "", y = perc, fill = .data[[col_name]])) +
    geom_bar(width = 1, stat = "identity", color = "white") +
    coord_polar("y", start = 0) +
    # Add text labels (positioned in the middle of the slices)
    geom_text(aes(label = labels),
              position = position_stack(vjust = 0.5),
              size = 4, color = "black") +
    labs(title = paste("Pie Chart:", col_name), x = NULL, y = NULL, fill = col_name) +
    theme_void() + # Pie charts usually don't need axis backgrounds; void theme is cleanest
    theme(plot.title = element_text(hjust = 0.5, face = "bold")) +
    scale_fill_brewer(palette = "Pastel1") # Use a pastel color palette
}

# Draw specific pie charts
pie_sex <- draw_pie_chart(Insurance_view, "sex")
pie_smoker <- draw_pie_chart(Insurance_view, "smoker")
pie_region <- draw_pie_chart(Insurance_view, "region")

# Display pie charts (Layout: two on top, one on bottom)
grid.arrange(pie_sex, pie_smoker, pie_region,
             layout_matrix = rbind(c(1, 2), c(3, 3)))
```

### 2.3. Bivariate Analysis

```{r}
# ============================
# Bivariate Analysis
# ============================

# ---- Feature Correlation ----


# compute correlation coefficients of "annual_medical_cost" to other features
cor_results <- sapply(c(num_cols, discrete_cols), function(v) {
  cor(Insurance[[v]], 
      Insurance$charges, use = "complete.obs")
})

cor_table <- data.frame(
  feature = names(cor_results),
  corr = as.numeric(cor_results)
)

# Sorting by "abs(corr)"
cor_table <- cor_table[order(abs(cor_table$corr), decreasing = TRUE), ]

# correlation coefficients table
cor_table


# ---- Correlation Heatmap ----
#top20_feature <- cor_table[1:20, ]$feature


corr_mat <- cor(Insurance[cor_table$feature], use = "complete.obs")

corrplot(corr_mat, method = "color",
         tl.cex = 0.5,
         tl.col = "black",
         number.cex = 0.4,
         addCoef.col = "black")

```

```{r}
# ============================
# 1. Categorical Variables vs Charges
# Use Boxplot to visualize distribution and outliers
# ============================

# Define a plotting function to avoid code duplication
plot_box <- function(data, x_col, y_col, fill_color) {
  # Use as.factor() to force the x-axis variable into a categorical factor
  # This allows scale_fill_brewer to work and ensures the Boxplot groups correctly
  ggplot(data, aes(x = as.factor(.data[[x_col]]), 
                   y = .data[[y_col]], 
                   fill = as.factor(.data[[x_col]]))) +
    
    geom_boxplot(alpha = 0.7, outlier.shape = NA) + 
    stat_summary(fun = mean, geom = "point", shape = 23, size = 3, fill = "white") + 
    
    # Manually set the x-label back to the original column name; otherwise, it would display "as.factor(children)"
    labs(title = paste(x_col, "vs", y_col), 
         x = x_col, 
         y = y_col, 
         fill = x_col) + # Set legend title
    
    theme_minimal() +
    theme(legend.position = "none") + # Boxplots usually don't need a legend since the x-axis is already labeled
    scale_fill_brewer(palette = fill_color)
}

# Re-run plotting
p1 <- plot_box(Insurance_view, "smoker", "charges", "Set1")
p2 <- plot_box(Insurance_view, "sex", "charges", "Pastel1")
p3 <- plot_box(Insurance_view, "region", "charges", "Set2")
# This line should work correctly now because 'children' is converted to a factor
p4 <- plot_box(Insurance_view, "children", "charges", "Spectral") 

grid.arrange(p1, p2, p3, p4, ncol = 2)
```

### 2.4 Analysis of Interaction Effects

```{r}
Insurance_view$log_charges <- log1p(Insurance_view$charges)
# ============================
# 1. BMI * Smoker 
# ============================

p_bmi_smoker <- ggplot(Insurance_view, aes(x = bmi, y = charges, color = smoker)) +
  geom_point(alpha = 0.5, size = 1) +
  geom_smooth(method = "lm", se = FALSE, size = 1.2) +
  scale_color_manual(values = c("yes" = "#c0392b", "no" = "#2980b9")) +
  labs(title = "1. Interaction: BMI * Smoker",
       x = "BMI", y = "Charges") +
  theme_minimal() +
  theme(legend.position = "top")


p_bmi_smoker
```

```{r}
# ============================
# 1. BMI * Smoker 
# ============================
log_p_bmi_smoker <- ggplot(Insurance_view, aes(x = bmi, y = log_charges, color = smoker)) +
  geom_point(alpha = 0.5, size = 1) +
  geom_smooth(method = "lm", se = FALSE, size = 1.2) +
  scale_color_manual(values = c("yes" = "#c0392b", "no" = "#2980b9")) +
  labs(title = "1. Interaction: BMI * Smoker",
       x = "BMI", y = "Log1p(Charges)") +
  theme_minimal() +
  theme(legend.position = "top")
log_p_bmi_smoker
```

```{r}
# ============================
# 2. Age * Smoker 
# ============================
p_age_smoker <- ggplot(Insurance_view, aes(x = age, y = charges, color = smoker)) +
  geom_point(alpha = 0.5, size = 1) +
  geom_smooth(method = "lm", se = FALSE, size = 1.2) +
  scale_color_manual(values = c("yes" = "#c0392b", "no" = "#2980b9")) +
  labs(title = "2. Interaction: Age * Smoker",
       x = "Age", y = "Charges") +
  theme_minimal() +
  theme(legend.position = "top")

p_age_smoker
```

```{r}
# ============================
# 2. Age * Smoker 
# ============================
log_p_age_smoker <- ggplot(Insurance_view, aes(x = age, y = log_charges, color = smoker)) +
  geom_point(alpha = 0.5, size = 1) +
  geom_smooth(method = "lm", se = FALSE, size = 1.2) +
  scale_color_manual(values = c("yes" = "#c0392b", "no" = "#2980b9")) +
  labs(title = "2. Interaction: Age * Smoker",
       x = "Age", y = "Log1p(Charges)") +
  theme_minimal() +
  theme(legend.position = "top")

log_p_age_smoker
```

```{r}
# ============================
# Special Handling: To plot bmi:age, we need to group age
# ============================
# Divide age into three stages: Young (18-35), Middle (36-55), Senior (55+)
Insurance_view <- Insurance_view %>%
  mutate(age_group = cut(age, 
                         breaks = c(0, 35, 55, 100), 
                         labels = c("Young (18-35)", "Middle (36-55)", "Senior (55+)")))
# ============================
# 3. BMI * Age 
# ============================
# Here we use the age_group we just created for grouping
p_bmi_age <- ggplot(Insurance_view, aes(x = bmi, y = charges, color = age_group)) +
  geom_point(alpha = 0.3, size = 1) +
  geom_smooth(method = "lm", se = FALSE, size = 1.2) +
  scale_color_brewer(palette = "Dark2") + # Use high-contrast colors
  labs(title = "3. Interaction: BMI * Age (Grouped)",
       x = "BMI", y = "Charges", color = "Age Group") +
  theme_minimal() +
  theme(legend.position = "top")

p_bmi_age
```

```{r}
# ============================
# 3. BMI * Age 
# ============================
# Here we use the age_group we just created for grouping
p_bmi_age <- ggplot(Insurance_view, aes(x = bmi, y = log_charges, color = age_group)) +
  geom_point(alpha = 0.3, size = 1) +
  geom_smooth(method = "lm", se = FALSE, size = 1.2) +
  scale_color_brewer(palette = "Dark2") + # 使用高對比顏色
  labs(title = "3. Interaction: BMI * Age (Grouped)",
       x = "BMI", y = "Log1p(Charges)", color = "Age Group") +
  theme_minimal() +
  theme(legend.position = "top")

p_bmi_age
```

## 3. Method

-   Data Preprocessing

```{r}

# ============================
# Data Preprocessing
# ============================


# data frame with all avaliable feature
df_raw <- Insurance

# Convert all character columns to factors
for (v in names(df_raw)) {
  if (is.character(df_raw[[v]])) {
    df_raw[[v]] <- factor(df_raw[[v]])
  }
}

# convert 0/1 columns to factors
discrete_cols <- c()

non_char_cols <- names(df_raw)[sapply(df_raw, is.numeric)]
num_cols <- setdiff(non_char_cols, discrete_cols)

# ---- One-hot encoding (dummy variables) ----
 # This turns each factor into 0/1 columns
df_ori <- model.matrix(~ ., data = df_raw)

# Final cleaned numeric dataset
df_ori <- as.data.frame(df_ori)
# convert col names to valid names
names(df_ori) <- make.names(names(df_ori))



# View origininal dataset
str(df_ori)
head(df_ori)

# ---- Data Transformation ----
df_trans <- df_ori

for(v in num_cols) {
  if(skewness(df_trans[,v]) > 1) {
    # log1p transform
    df_trans[,v] <- log1p(df_trans[,v])
    
    before <- ggplot(df_ori, aes(x = .data[[v]])) + 
    geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "steelblue", alpha = 0.7) +
    theme(axis.text.x = element_text(size = 4)) +
    theme_minimal() + 
    labs(
      title = paste("Before Transformation:", v)
    )
    after <- ggplot(df_trans, aes(x = .data[[v]])) + 
    geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "steelblue", alpha = 0.7) +
    theme(axis.text.x = element_text(size = 4)) +
    theme_minimal() +
    labs(
      title = paste("After Transformation:", v),
      x = paste0("log1p(", v, ")")      # x label 直接寫清楚
    )
    grid.arrange(grobs = list(before, after), ncol = 2)

  }
}

```

We aim to categorical data perform dummy one-hot encoding. Then, for the
right-skew numerical data, we transform data by log1p().

-   Data Splitting

```{r split-data}

set.seed(123)

# Number of observations
n <- nrow(df_trans)

# 80% for training, 20% for test
train_idx <- sample(seq_len(n), size = 0.8 * n)

train <- df_trans[train_idx, ]
test  <- df_trans[-train_idx, ]

train_ori <- df_ori[train_idx, ]
test_ori  <- df_ori[-train_idx, ]
```

We randomly split the cleaned dataset into a 80% training set and a 20%
test set using a fixed random seed to ensure reproducibility. All models
in this section are trained on the training set and evaluated on the
test set for fair comparison.

-   Some Useful Function

```{r}
# RMSE
computeRMSE <- function(y_pred, y_true, transform = FALSE) {
  if (transform == TRUE) {
    y_pred <- expm1(y_pred)
    y_true <- expm1(y_true)
  }
  rmse <- sqrt(mean((y_pred - y_true)^2))
    
  return(rmse)
}

# R^2
computeR2 <- function(y_pred, y_true, transform = FALSE) {
  if (transform == TRUE) {
    y_pred <- expm1(y_pred)
    y_true <- expm1(y_true)
  }
  
  SSE <- sum((y_true - y_pred)^2)
  SST <- sum((y_true - mean(y_true))^2)
  
  R2 <- 1 - SSE/SST
  
  return(R2)
}

```

### 3.1 Regression Model

#### 3.1.1 Linear regression model(Baseline)

```{r baseline-ols}
# Response variable
#response_var <- "annual_medical_cost"
response_var <- "charges"
# Variables that would leak cost information — must be excluded
# sus_vars <- c("annual_premium", "monthly_premium", "claims_count", "avg_claim_amount", "total_claims_paid")
leakage_vars <- c()


# Construct predictor set
predictor_vars <- setdiff(
  names(df_trans),
  c(response_var, leakage_vars)
)



# Build model formula
form_baseline <- as.formula(
  paste(response_var, "~", paste(predictor_vars, collapse = "+"))
)

# Fit OLS
lm_full <- lm(form_baseline, data = train)
summary(lm_full)

# Predictions
train_pred_lm <- predict(lm_full, newdata = train)
test_pred_lm  <- predict(lm_full, newdata = test)

# Compute R^2
y_train <- train[[response_var]]
y_test  <- test[[response_var]]

R2_train_lm <- computeR2(train_pred_lm, y_train, TRUE)
#  1 - sum((y_train - train_pred_lm)^2) / sum((y_train - mean(y_train))^2)

R2_test_lm  <- computeR2(test_pred_lm, y_test, TRUE)
# 1 - sum((y_test - test_pred_lm)^2) / sum((y_test - mean(y_test))^2)

# Compute RMSE
RMSE_train_lm <- computeRMSE(train_pred_lm, y_train, TRUE)
RMSE_test_lm  <- computeRMSE(test_pred_lm, y_test, TRUE)

R2_train_lm; R2_test_lm
RMSE_train_lm; RMSE_test_lm
```

The baseline OLS model utilizes all transformed predictors to estimate
medical charges. The model achieves a training R\^2 = 0.53 and test R\^2
= 0.51, indicating that the linear structure explains only about half of
the variance in medical costs. The RMSE values=8284.48 on the training
set and 8644.52 on the test set show that substantial prediction error
remains.

Significant predictors include age, BMI, number of children, smoking
status, and certain regional indicators, all aligning with expected
relationships in healthcare expenditure. However, the large and uneven
residuals suggest the presence of nonlinear patterns and
heteroscedasticity that the linear model fails to capture. These
findings imply that more flexible modeling approaches may be required to
achieve improved predictive performance.

#### 3.1.2 Linear regression model with Lasso and CV

```{r lasso-cv-setup}

# Construct design matrices (remove intercept)
x_train_base <- model.matrix(form_baseline, data = train)[, -1]
x_test_base  <- model.matrix(form_baseline, data = test)[, -1]
```

```{r lasso-cv-fit}
set.seed(123)

cv_lasso_base <- cv.glmnet(
  x = x_train_base,
  y = y_train,
  alpha = 1,        # Lasso
  nfolds = 10,
  standardize = TRUE
)

lambda_min_base <- cv_lasso_base$lambda.min
lambda_min_base
```

In the Lasso model with 10-fold cross-validation, the selected penalty
parameter is very small:

$\lambda_{\min} \approx$ 0.0013

Such a small λ implies very weak regularization, so the Lasso solution
is expected to be very close to the OLS solution. In other words,
cross-validation suggests that the data do not benefit much from strong
shrinkage on the coefficients under this feature scaling. As a result,
the fitted coefficients and overall performance of the Lasso model are
almost the same as those of the baseline linear regression.

```{r lasso-cv-performance}
# Predictions for CV-selected lambda.min
train_pred_cv_base <- predict(cv_lasso_base, newx = x_train_base, s = "lambda.min")
test_pred_cv_base  <- predict(cv_lasso_base, newx = x_test_base,  s = "lambda.min")

# Compute R^2
R2_train_cv_base <- computeR2(y_train, train_pred_cv_base, TRUE)
#  1 - sum((y_train - train_pred_cv_base)^2) / sum((y_train - mean(y_train))^2)

R2_test_cv_base  <- computeR2(y_test, test_pred_cv_base, TRUE)
#  1 - sum((y_test - test_pred_cv_base)^2) / sum((y_test - mean(y_test))^2)

# Compute RMSE
RMSE_train_cv_base <- computeRMSE(y_train, train_pred_cv_base, TRUE)
RMSE_test_cv_base  <- computeRMSE(y_test, test_pred_cv_base, TRUE)

R2_train_cv_base; R2_test_cv_base
RMSE_train_cv_base; RMSE_test_cv_base
```

The model achieves $R^2_{\text{train}}$ = 0.636 and $R^2_{\text{test}}$
= 0.568, which are similar to the baseline OLS model. The RMSE values
(training: 8244.83, test: 8591.93) show no meaningful reduction in
prediction error.

Because the dataset contains a modest number of predictors and exhibits
notable nonlinear patterns, Lasso does not address the underlying model
misspecification. Residual diagnostics continue to display curved
structures, indicating that the linear assumption is insufficient.

#### 3.1.3 Linear regression model with Lasso and BIC

```{r fit full Lasso path & compute BIC}
# Fit full Lasso path on baseline features
lasso_base <- glmnet(
  x = x_train_base,
  y = y_train,
  alpha = 1,
  standardize = TRUE
)

# Function to compute BIC for each lambda
calc_bic <- function(fit, x, y) {
  n <- length(y)
  y_hat <- predict(fit, newx = x)
  rss <- colSums((y - y_hat)^2)
  df  <- fit$df  # number of non-zero coefficients
  bic <- n * log(rss / n) + df * log(n)
  
  return(bic)
}

bic_base <- calc_bic(lasso_base, x_train_base, y_train)

lambda_bic_base <- lasso_base$lambda[which.min(bic_base)]
lambda_bic_base
plot(log(lasso_base$lambda), bic_base, type = "l",
     xlab = "log(lambda)", ylab = "BIC")
abline(v = log(lambda_bic_base), lty = 2)
```

To select the optimal regularization strength, we evaluated the full
Lasso solution path using the Bayesian Information Criterion (BIC). The
λ that minimizes BIC is:

$\lambda_{\text{BIC}}$ = 0.001315

This value is extremely small, indicating that BIC favors very weak
regularization. Consequently, the selected model retains almost all
coefficients and behaves nearly identically to the CV-selected Lasso
model and even to the baseline OLS model.

```{r use BIC λ to evaluate performance}
# Predictions using BIC-selected lambda
train_pred_bic_base <- predict(lasso_base, newx = x_train_base, s = lambda_bic_base)
test_pred_bic_base  <- predict(lasso_base, newx = x_test_base,  s = lambda_bic_base)

# Compute R^2
R2_train_bic_base <- computeR2(y_train, train_pred_bic_base, TRUE)
# 1 - sum((y_train - train_pred_bic_base)^2) / sum((y_train - mean(y_train))^2)

R2_test_bic_base  <- computeR2(y_test, test_pred_bic_base, TRUE)
# 1 - sum((y_test - test_pred_bic_base)^2) / sum((y_test - mean(y_test))^2)

# Compute RMSE
RMSE_train_bic_base <- computeRMSE(y_train, train_pred_bic_base, TRUE)
RMSE_test_bic_base  <- computeRMSE(y_test, test_pred_bic_base, TRUE)

R2_train_bic_base; R2_test_bic_base
RMSE_train_bic_base; RMSE_test_bic_base
```

```         
•   Training R² = 0.636

•   Test R² = 0.568

•   Training RMSE = 8244.83

•   Test RMSE = 8591.93
```

Compared with the CV-selected Lasso model, the BIC-selected λ imposes
stronger regularization, resulting in a more sparse model with higher
bias. This leads to lower R² and higher RMSE on both training and test
sets.

```{r baseline-summary}


results_baseline <- data.frame(
  Model = c("OLS baseline",
            "Lasso (CV λ_min)",
            "Lasso (BIC)"),
  R2_test = c(R2_test_lm, R2_test_cv_base, R2_test_bic_base),
  RMSE_test = c(RMSE_test_lm, RMSE_test_cv_base, RMSE_test_bic_base)
)

results_baseline
```

Among the three baseline linear models, the Lasso versions perform
slightly better than OLS. The OLS model reaches an R² of about 0.51,
while Lasso improves it to 0.568 and reduces RMSE from 8644 to 8592,
meaning more accurate predictions. Interestingly, both CV and BIC choose
the same λ, so their results are identical. Overall, Lasso helps, but
the improvement is modest, suggesting that a simple linear model might
not fully capture the structure of the data and more flexible methods
may be needed later.

```{r}

# --- OLS (baseline) ---
results <- data.frame(preds = expm1(test_pred_lm), actual = expm1(y_test))
results$residuals <- results$actual - results$preds

# Prediction vs Actual
ggplot(results, aes(x = preds, y = actual)) +
  geom_point(alpha = 0.5) +
  geom_abline(color = "red") +
  labs(title = "OLS Prediction vs Actual")

# Prediction vs Residuals
ggplot(results, aes(x = preds, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red") +
  labs(title = "OLS Prediction vs Residuals")
```

1.  

From the OLS prediction plot, the model captures the general upward
trend, but once the medical charges become large (around 30,000 and
above), the predictions start to fall noticeably below the 45-degree
reference line. This indicates that OLS systematically underestimates
high-cost patients.

The residual plot shows the issue even more clearly. Instead of being
randomly scattered around zero, the residuals form a curved, structured
pattern. Lower-cost observations tend to have positive residuals, while
higher-cost ones have increasingly negative residuals. This pattern
suggests that the model is missing important nonlinear relationships,
and its linear structure is not flexible enough to explain the full
behavior of medical costs.

OLS captures the overall trend but fails to model the nonlinear increase
in costs, leading to systematic underestimation for high-charge patients
and a characteristic curved residual pattern.

```{r}
# --- CV ---

test_pred_cv_base <- as.numeric(test_pred_cv_base)

results <- data.frame(preds = expm1(test_pred_cv_base), actual = expm1(y_test))
results$residuals <- results$actual - results$preds

# Prediction vs Actual
ggplot(results, aes(x = preds, y = actual)) +
  geom_point(alpha = 0.5) +
  geom_abline(color = "red") +
  labs(title = "Best CV Prediction vs Actual")

# Prediction vs Residuals
ggplot(results, aes(x = preds, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red") +
  labs(title = "Best CV Prediction vs Residuals")
```

2.  

In the CV-selected Lasso model, the prediction plot still follows the
overall trend of the actual charges, but the pattern is very similar to
OLS. Once the true medical cost becomes large, the predictions remain
noticeably below the 45-degree reference line. This means that even
after applying Lasso regularization, the model still underestimates
high-cost patients.

The residual plot confirms this. The residuals do not scatter randomly
around zero; instead, they show a clear downward pattern as predictions
increase. This indicates that errors are systematically related to the
scale of the predicted cost. In other words, the model is still missing
key nonlinear relationships, and Lasso shrinkage alone is not enough to
correct the structural bias.

The Lasso model chosen by cross-validation performs slightly better
numerically, but the core issue remains—systematic underestimation for
high-spending individuals and a strong nonlinear residual pattern,
suggesting that a purely linear model is not flexible enough.

```{r}
# --- BIC ---
test_pred_bic_base <- as.numeric(test_pred_bic_base)

results <- data.frame(preds = expm1(test_pred_bic_base), actual = expm1(y_test))
results$residuals <- results$actual - results$preds

# Prediction vs Actual
ggplot(results, aes(x = preds, y = actual)) +
  geom_point(alpha = 0.5) +
  geom_abline(color = "red") +
  labs(title = "Best BIC Prediction vs Actual")

# Prediction vs Residuals
ggplot(results, aes(x = preds, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red") +
  labs(title = "Best BIC Prediction vs Residuals")
```

3.  

The BIC-selected Lasso model shows prediction patterns almost identical
to the CV-selected Lasso, which makes sense because both methods choose
similar λ values. The model captures the general increasing trend, but
like the other models, it consistently under-predicts high-cost cases,
and the residual plot again shows a clear downward pattern. This
indicates remaining nonlinearity in the data that linear
models—including penalized ones—cannot fully handle.

Summary

Across the baseline OLS model and the Lasso models selected by CV and
BIC, the overall predictive performance remains limited. Although Lasso
slightly improves R² and reduces RMSE compared to OLS, all three models
show the same systematic issue: they consistently underestimate high
medical charges, and their residuals reveal a strong nonlinear pattern
that linear models cannot capture. This suggests that medical cost data
contains complex relationships—likely interactions or nonlinear
effects—that are beyond the capability of linear or penalized linear
models.

#### 3.1.2

### subset selection

```{r}
train_set = train[,-1]
test_set = test[,-1]
```

```{r}
set.seed(123)
regfit.full = regsubsets(charges ~ ., data = train_set)
reg1.summary = summary(regfit.full)
reg1.summary
```

```{r}
#par(mfrow=c(1,2))
index = which.max(reg1.summary$adjr2)
cat("\nSize:", index, "\n")
cat("Adjusted RSq:", reg1.summary$adjr2[index],"\n")
coef(regfit.full,index)
plot(reg1.summary$adjr2, xlab='Number of Variables',ylab='Adjusted RSq',type='b', pch=19)
points(index,reg1.summary$adjr2[index],col = "blue", pch=19)


index = which.min(reg1.summary$bic)
cat("\nSize:", index, "\n")
cat("BIC:", reg1.summary$bic[index],"\n")
coef(regfit.full,index)
plot(reg1.summary$bic, xlab='Number of Variables',ylab='BIC',type='b', pch=19)
points(index,reg1.summary$bic[index],col = "blue", pch=19)

```

```{r}
test.mat = model.matrix(charges~.,data = test_set)
test.errors = rep(NA,8)
for(i in 1:8){
  coefi = coef(regfit.full,id=i)
  intercept = as.vector(coefi)[1]
  pred = test.mat[,names(coefi)] %*% coefi
  test.errors[i] = sqrt(mean((test_set$charges - pred)^2))
}
min.index = which.min(reg1.summary$rss)
cat("\nSize:", min.index, "\n")
cat("Train RMSE:", sqrt(reg1.summary$rss[min.index]/n),"\n")
coef(regfit.full,id=min.index)
plot(sqrt((reg1.summary$rss)/n), xlab='Number of Variables',ylab='Train RMSE',type='b',pch=19)
points(min.index, sqrt(reg1.summary$rss[min.index]/n),col = "blue", pch=19)

min.index = which.min(test.errors)
cat("\nSize:", min.index, "\n")
cat("Test RMSE:", test.errors[min.index],"\n")
coef(regfit.full,id=min.index)
plot(1:8, test.errors,xlab='Number of variables', ylab='Test RMSE',type="b", pch=19)
points(min.index, test.errors[min.index],col = "blue", pch=19)
```

#### forward selection

```{r}
set.seed(123)
regfit.fwd = regsubsets(charges~.,data = train_set,method = 'forward')
reg.fwd.summary = summary(regfit.fwd)
reg.fwd.summary
```

```{r}
index = which.max(reg.fwd.summary$adjr2)
cat("\nSize:", index, "\n")
cat("Adjusted RSq:", reg.fwd.summary$adjr2[index],"\n")
coef(regfit.fwd,index)
plot(reg.fwd.summary$adjr2, xlab='Number of Variables',ylab='Adjusted RSq',type='b', pch=19)
points(index,reg.fwd.summary$adjr2[index],col = "blue", pch=19)


index = which.min(reg.fwd.summary$bic)
cat("\nSize:", index, "\n")
cat("BIC:", reg.fwd.summary$bic[index],"\n")
coef(regfit.fwd,index)
plot(reg.fwd.summary$bic, xlab='Number of Variables',ylab='BIC',type='b', pch=19)
points(index,reg.fwd.summary$bic[index],col = "blue", pch=19)
```

```{r}
test.mat = model.matrix(charges~.,data = test_set)
test.errors = rep(NA,8)
for(i in 1:8){
  coefi = coef(regfit.fwd,id=i)
  intercept = as.vector(coefi)[1]
  pred = test.mat[,names(coefi)] %*% coefi
  test.errors[i] = sqrt(mean((test_set$charges - pred)^2))
}
min.index = which.min(reg.fwd.summary$rss)
cat("\nSize:", min.index, "\n")
cat("Train RMSE:", sqrt(reg.fwd.summary$rss[min.index]/n),"\n")
coef(regfit.fwd,id=min.index)
plot(sqrt((reg.fwd.summary$rss)/n), xlab='Number of Variables',ylab='Train RMSE',type='b',pch=19)
points(min.index, sqrt(reg.fwd.summary$rss[min.index]/n),col = "blue", pch=19)

min.index = which.min(test.errors)
cat("\nSize:", min.index, "\n")
cat("Test RMSE:", test.errors[min.index],"\n")
coef(regfit.fwd,id=min.index)
plot(1:8, test.errors,xlab='Number of variables', ylab='Test RMSE',type="b", pch=19)
points(min.index, test.errors[min.index],col = "blue", pch=19)
```

#### backward selection

```{r}
set.seed(123)
regfit.bwd = regsubsets(charges~.,data = train_set,method = 'backward')
reg.bwd.summary = summary(regfit.bwd)
reg.bwd.summary
```

```{r}
index = which.max(reg.bwd.summary$adjr2)
cat("\nSize:", index, "\n")
cat("Adjusted RSq:", reg.bwd.summary$adjr2[index],"\n")
coef(regfit.bwd,index)
plot(reg.bwd.summary$adjr2, xlab='Number of Variables',ylab='Adjusted RSq',type='b', pch=19)
points(index,reg.bwd.summary$adjr2[index],col = "blue", pch=19)


index = which.min(reg.bwd.summary$bic)
cat("\nSize:", index, "\n")
cat("BIC:", reg.bwd.summary$bic[index],"\n")
coef(regfit.bwd,index)
plot(reg.bwd.summary$bic, xlab='Number of Variables',ylab='BIC',type='b', pch=19)
points(index,reg.bwd.summary$bic[index],col = "blue", pch=19)
```

```{r}
test.mat = model.matrix(charges~.,data = test_set)
test.errors = rep(NA,8)
for(i in 1:8){
  coefi = coef(regfit.bwd,id=i)
  intercept = as.vector(coefi)[1]
  pred = test.mat[,names(coefi)] %*% coefi
  test.errors[i] = sqrt(mean((test_set$charges - pred)^2))
}
min.index = which.min(reg.bwd.summary$rss)
cat("\nSize:", min.index, "\n")
cat("Train RMSE:", sqrt(reg.bwd.summary$rss[min.index]/n),"\n")
coef(regfit.bwd,id=min.index)
plot(sqrt((reg.bwd.summary$rss)/n), xlab='Number of Variables',ylab='Train RMSE',type='b',pch=19)
points(min.index, sqrt(reg.bwd.summary$rss[min.index]/n),col = "blue", pch=19)

min.index = which.min(test.errors)
cat("\nSize:", min.index, "\n")
cat("Test RMSE:", test.errors[min.index],"\n")
coef(regfit.bwd,id=min.index)
plot(1:8, test.errors,xlab='Number of variables', ylab='Test RMSE',type="b", pch=19)
points(min.index, test.errors[min.index],col = "blue", pch=19)
```

#### result

Linear regression model with 7 predictors by feature selection:

```{r}
predictors = setdiff(
  names(train),
  c('charges','regionnorthwest')
)
form_fs <- as.formula(
  paste('charges', "~", paste(predictors, collapse = "+"))
)
lm_fs <- lm(form_fs,data = train)
summary(lm_fs)

# Predictions
train_pred <- predict(lm_fs, newdata = train)
test_pred  <- predict(lm_fs, newdata = test)

# Compute R^2
y_train <- train$charges
y_test  <- test$charges

R2_train <- 1 - sum((y_train - train_pred)^2) /
                    sum((y_train - mean(y_train))^2)
R2_test  <- 1 - sum((y_test - test_pred)^2) /
                    sum((y_test - mean(y_test))^2)

# Compute RMSE
RMSE_train <- sqrt(mean((y_train - train_pred)^2))
RMSE_test  <- sqrt(mean((y_test - test_pred)^2))

cat('Train R^2:',R2_train,'\n')
cat('Test R^2:',R2_test,'\n')
cat('Train RMSE:',RMSE_train,'\n')
cat('Test RMSE:',RMSE_test,'\n')
```

```{r}
results <- data.frame(preds = test_pred, actual = test_set$charges )
results$residuals <- results$actual - results$preds

# Prediction vs Actual
ggplot(results, aes(x = preds, y = actual)) +
  geom_point(alpha = 0.5) +
  geom_abline(color = "red") +
  labs(title = "Prediction vs Actual")

# Prediction vs Residuals
ggplot(results, aes(x = preds, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red") +
  labs(title = "Prediction vs Residuals")
```

### 3.2 Tree-based Model

#### 3.2.1 Random Forest

```{r}

# ==========================================
#  Random Forest model with CV
# ==========================================

set.seed(321)

# --- Variable Setup ---
# response variable (target)
response_var <- c("charges")

# Predictor variable set
predictor_vars <- setdiff(
  names(df_ori),
  c(response_var)
)

# Construct model formula
form_rf <- as.formula(
  paste(response_var, "~", paste(predictor_vars, collapse = "+"))
)

y_train <- train_ori[,response_var]
y_test <- test_ori[,response_var]

# --- Training model ---

# number of tree (ntree)
B <- 200
# number of predictors
p <- ncol(train_ori) - 1

# rf <- randomForest(form_rf, data = train_raw, 
#                       mtry = p/2, ntree = B, importance = TRUE)

# 5-fold Cross-Validation
cv_ctrl <- trainControl(method = "cv", number = 5)

# --- training random forest with CV ---
rf_cv <- train(
  form_rf, 
  data = train_ori,
  method = "rf",
  trControl = cv_ctrl,
  tuneLength = p-1, # try mtry = 1 to (p-1)
  ntree = B, # B trees
)

# The result of best random forest
rf_cv

# Variable Importance
varImp(rf_cv)

```

The best number of features for the random forest is 5, which has the
highest $R^2$ and the lowest RMSE.

```{r}

# ==========================================
#  Computing metrics
# ==========================================

# Predictions
rf_pred_train <- predict(rf_cv, newdata = train_ori)
rf_pred_test <- predict(rf_cv, newdata = test_ori)

# Train RMSE & R2
rf_train_rmse <- computeRMSE(rf_pred_train, y_train)
rf_train_R2 <- computeR2(rf_pred_train, y_train)

# Test RMSE & R2 (the most important metric)
rf_test_rmse <- computeRMSE(rf_pred_test, y_test)
rf_test_R2 <- computeR2(rf_pred_test, y_test)

# Output the result
cat("Training RMSE:", rf_train_rmse, "\n")
cat("Testing RMSE: ", rf_test_rmse, "\n")
cat("Training R2:  ", rf_train_R2, "\n")
cat("Testing R2:   ", rf_test_R2, "\n")


```

```{r}

# ==========================================
#  Prediction and Residual Plot
# ==========================================



# Build plot DataFrame
plot_data <- data.frame(
  Predicted = rf_pred_test,
  #  Residuals = actual - predicts
  actual = y_test,
  Residuals = y_test - rf_pred_test
)

# --- plots ---
# 1. actual vs predicts (Ideally, the points should follow 45-degree line.)
ggplot(plot_data, aes(x = Predicted, y = actual)) +
  geom_point(alpha = 0.5, color = "darkblue") + # scatter point
  geom_abline(yintercept = 0, color = "red", linetype = "dashed") + # 45-degree
  labs(title = "Prediction vs Actual", 
    x = "Predicted Charges",
    y = "True Charges") +
  theme_minimal()

# 2. Residual Distribution (The points should follow horizontal line.)
ggplot(plot_data, aes(x = Predicted, y = Residuals)) +
  geom_point(alpha = 0.5, color = "darkblue") + # scatter point
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") + # horizontal
  labs(
    title = "Residual Plot (Random Forest)",
    x = "Predicted Charges",
    y = "Residuals (Actual - Predicted)"
  ) +
  theme_minimal()
```

#### 3.2.2 XGBoost

```{r}
# ==========================================
#  XGBoost with CV
# ==========================================

dtrain <- xgb.DMatrix(data = as.matrix(train_ori %>% select(-charges)), label = train_ori$charges)
params <- list(
  booster = "gbtree",
  objective = "reg:tweedie", # Importance! This is the target function for regression
  tweedie_variance_power = 1.1,
  eval_metric = "rmse",       # evaluate model by RMSE
  eta = 0.05,                 # Learning rate
  max_depth = 6,              # max depth of each tree
  subsample = 0.8,            # Row subsampling rate
  colsample_bytree = 1,       # Feature subsampling rate
  gamma = 0.05,                   
  min_child_weight = 5        # Min sum of instance weight needed in a child node
)

# --- Training Model ---

# Using CV to find best rounds (nrounds) and avoid overfitting
cv_model <- xgb.cv(
  set.seed(42),
  params = params,
  data = dtrain,
  nrounds = 1000,        # Upper bound for number of boosting rounds
  nfold = 5,             # 5-fold cross validation
  early_stopping_rounds = 10, # Stop if no improvement for 10 rounds
  verbose = 1,           # Display training progress
  print_every_n = 100
)

# Extract the best number of boosting rounds

best_nrounds <- cv_model$best_iteration


# Train the final XGBoost model with optimal parameters
final_model <- xgb.train(
  set.seed(42),
  params = params,
  data = dtrain,
  nrounds = best_nrounds
)
```

```{r}
dtest <- xgb.DMatrix(data = as.matrix(test_ori %>% select(-charges)), label = test_ori$charges)
pred <- predict(final_model, dtest)

# --- Accuracy Validation ---
# Compute RMSE
rmse <- computeRMSE(pred, y_test)

cat("--- Model Evaluation ---\n")
cat("Testing RMSE (USD):", rmse, "\n")

# Compute R-squared
r_sq <- computeR2(pred, y_test)
cat("Testing R-squared:", r_sq, "\n")
```

```{r}
# 1. Prediction vs Actual (修正 geom_abline)
ggplot(results, aes(x = preds, y = actual)) +
  geom_point(alpha = 0.5, color = "darkblue") +  # 加上顏色區分
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed", size = 1) + # 強制設定斜率為1
  labs(title = "XGBoost: Prediction vs Actual",
       x = "Predicted Charges (USD)",
       y = "Actual Charges (USD)") +
  theme_minimal()

# 2. Residual Plot (修正 geom_hline)
ggplot(results, aes(x = preds, y = residuals)) +
  geom_point(alpha = 0.5, color = "darkblue") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed", size = 1) + # 明確指定水平線
  labs(title = "XGBoost: Residual Analysis",
       x = "Predicted Charges",
       y = "Residuals (Actual - Predicted)") +
  theme_minimal()
```

```{r}
# 1. Compute importance matrix
importance_matrix <- xgb.importance(feature_names = colnames(dtrain), model = final_model)

# 2. Watch parameters
print(importance_matrix)

# 3. Draw plot
xgb.plot.importance(importance_matrix)
```

```{r}
rho <- 1.1 

xgb_df <- data.frame(
  actuals = test_ori$charges,
  preds = pred
)

xgb_df$raw_resid <- xgb_df$actuals - xgb_df$preds

xgb_df$pearson_resid <- (xgb_df$actuals - xgb_df$preds) / (xgb_df$preds^(rho / 2))

p1 <- ggplot(xgb_df, aes(x = preds, y = raw_resid)) +
  geom_point(alpha = 0.5, color = "black") +
  geom_hline(yintercept = 0, color = "red") +
  labs(title = "Raw Residuals (Original)",
       x = "Predicted Values", y = "y - y_hat") +
  theme_minimal()

p2 <- ggplot(xgb_df, aes(x = preds, y = pearson_resid)) +
  geom_point(alpha = 0.5, color = "blue") +
  geom_hline(yintercept = 0, color = "red") +
  labs(title = "Pearson Residuals (Corrected)",
       subtitle = paste0("Standardized by variance power rho = ", rho),
       x = "Predicted Values", y = "Pearson Residuals") +
  theme_minimal()

p1
p2
```

```{r}
# 1. Define a threshold for "severe underestimation" (e.g., residual > 10 or 20)
# These are cases where the model predicted a low cost, but the actual cost was very high.
outliers <- xgb_df[xgb_df$pearson_resid > 10, ]

# 2. Check the number of these outliers
print(paste("Number of severely underestimated cases:", nrow(outliers)))

# 3. Print and inspect these cases (viewing the first few rows)
# You should focus on observing the features of these individuals (Age, BMI, Region, Smoker?).
# Look for any common patterns that the model might have missed.
head(outliers)

# 4. Compare the average features of "normal cases" vs. "outliers"
# This helps you identify feature discrepancies.
summary(xgb_df)           # Statistics for the entire dataset
summary(outliers)         # Statistics for the outliers
```

```{r}
# 定義異常值與正常值
outliers_idx <- which(xgb_df$pearson_resid > 10)
outlier_data <- df_ori[outliers_idx, ]
normal_data <- df_ori[-outliers_idx, ]

# 建立一個比較函數，直接看比例差異
compare_props <- function(col_name) {
  cat("\n--- Comparison for:", col_name, "---\n")
  
  # 計算 Outlier 組的比例
  prop_outlier <- prop.table(table(outlier_data[[col_name]]))
  cat("Outlier Group Proportions:\n")
  print(round(prop_outlier, 3))
  
  # 計算 Normal 組的比例
  prop_normal <- prop.table(table(normal_data[[col_name]]))
  cat("Normal Group Proportions:\n")
  print(round(prop_normal, 3))
}

# 執行比較 (針對類別變數)
compare_props("smoker")
compare_props("sex")
compare_props("regionnorthwest") # 依此類推其他區域
compare_props("children")

# 針對數值變數 (Age, BMI) 檢查平均數或中位數
cat("\n--- Numeric Comparison ---\n")
cat("Mean BMI - Outliers:", mean(outlier_data$bmi), " vs Normal:", mean(normal_data$bmi), "\n")
cat("Mean Age - Outliers:", mean(outlier_data$age), " vs Normal:", mean(normal_data$age), "\n")
```

```{r}
# 1. 標記 Outlier 與 Normal
# 假設 xgb_df 已經存在 (來自 PDF 第 66 頁程式碼)
outliers_idx <- which(xgb_df$pearson_resid > 10) 

# 建立繪圖用的資料集 (複製一份 df_ori 以免動到原檔)
plot_df <- df_ori 
plot_df$Group <- "Normal"
plot_df$Group[outliers_idx] <- "Outlier"

# 將 Group 轉為因子，確保 Outlier 顯示顏色一致
plot_df$Group <- factor(plot_df$Group, levels = c("Normal", "Outlier"))
```

```{r}
# 定義畫圓餅圖的函數
draw_comparison_pie <- function(data, variable, title_text) {
  
  # 1. 計算百分比
  pie_data <- data %>%
    group_by(Group, !!sym(variable)) %>%
    summarise(count = n(), .groups = 'drop') %>%
    group_by(Group) %>%
    mutate(prop = count / sum(count)) %>%
    mutate(label = sprintf("%1.0f%%", prop * 100)) # 產生百分比標籤
  
  # 2. 繪圖
  # 修改重點：fill = as.factor(!!sym(variable)) 
  # 這會強制把 0/1 轉成類別，解決 scale_fill_brewer 的報錯
  ggplot(pie_data, aes(x = "", y = prop, fill = as.factor(!!sym(variable)))) +
    geom_bar(width = 1, stat = "identity", color = "white") +
    coord_polar("y", start = 0) +
    facet_wrap(~Group) + 
    geom_text(aes(label = label), 
              position = position_stack(vjust = 0.5), size = 3) +
    # 設定圖例標題為變數名稱，並指定顏色盤
    labs(title = title_text, x = NULL, y = NULL, fill = variable) +
    theme_void() + 
    theme(plot.title = element_text(hjust = 0.5, face = "bold"),
          legend.position = "bottom") +
    scale_fill_brewer(palette = "Pastel1") 
}
plot_df <- plot_df %>%
  mutate(region = case_when(
    regionnorthwest == 1 ~ "northwest",
    regionsoutheast == 1 ~ "southeast",
    regionsouthwest == 1 ~ "southwest",
    TRUE ~ "northeast"  # 如果以上都是 0，那就是基準組 northeast
  ))

# --- 重新執行繪圖 ---

# 1. 比較吸菸者 (Smoker)
p_pie_smoker <- draw_comparison_pie(plot_df, "smokeryes", "Smoker Distribution")

# 2. 比較地區 (Region) -這是新產生的 region 欄位
p_pie_region <- draw_comparison_pie(plot_df, "region", "Region Distribution")

# 3. 顯示圖表
p_pie_smoker
p_pie_region
```

```{r}
# 定義畫箱型圖的函數
draw_comparison_box <- function(data, variable, title_text) {
  ggplot(data, aes(x = Group, y = !!sym(variable), fill = Group)) +
    geom_boxplot(alpha = 0.7, outlier.shape = 21, outlier.fill = "white") +
    labs(title = title_text, x = "", y = variable) +
    theme_minimal() +
    theme(legend.position = "none", # 箱型圖不需要圖例，X軸已經很清楚
          plot.title = element_text(hjust = 0.5, face = "bold"),
          axis.text.x = element_text(size = 12, face = "bold")) +
    scale_fill_manual(values = c("Normal" = "#3498db", "Outlier" = "#e74c3c")) # 藍色 vs 紅色
}

# --- 產生箱型圖 ---

# 1. 比較 BMI
p_box_bmi <- draw_comparison_box(plot_df, "bmi", "BMI Distribution")

# 2. 比較年齡 (Age)
p_box_age <- draw_comparison_box(plot_df, "age", "Age Distribution")

# 顯示圖表
grid.arrange(p_box_bmi, p_box_age, ncol = 2)
```

##### Conclusion on Xgboost Performance and Error Analysis

The residual analysis confirms that the XGBoost model using the Tweedie
objective successfully captures the general variance of medical costs.
However, Pearson residual diagnostics revealed a specific cluster of
under-predicted outliers ($n=32$).

A detailed inspection shows that these outliers do not possess
conspicuously higher risk factors compared to the general population.
Their average BMI ($\approx 30.5$) is similar to the normal group
($\approx 30.7$), and while they are slightly younger (mean age
$\approx 36$ vs $39$), their demographic profile does not explicitly
justify the extreme costs incurred.

Although the outlier group contains a slightly higher proportion of
smokers (28%) compared to the normal group (20%), the majority are
non-smokers. The model likely under-predicted these cases because their
available features (Average BMI, Younger Age) signaled moderate risk,
yet they incurred high actual expenses. This discrepancy suggests these
costs are driven by aleatoric uncertainty—likely due to random, acute
medical events (e.g., accidents or sudden critical illnesses) that
cannot be predicted by demographic data alone. The model has likely
reached its performance ceiling with the current feature set.

## 4. Application

Based on the predictive capabilities of our best-performing model
(XGBoost with Tweedie objective) and the insights gained from the
exploratory data analysis, we propose several practical applications for
the insurance industry and healthcare management:

### 4.1 Precision Pricing and Dynamic Premium Adjustment

Traditional actuarial tables often rely on broad demographic brackets.
Our model demonstrates that non-linear interactions (e.g., BMI \* Smoker
and Age \* BMI) significantly impact medical costs.

-   Implementation: Insurance companies can deploy this model to
    calculate Individualized Risk Scores. Instead of a flat rate for all
    "smokers," the premium can be dynamically adjusted based on the
    compounding risk of obesity and age.

-   Benefit: This leads to fairer pricing strategies where low-risk
    individuals are not subsidizing high-risk behaviors, potentially
    increasing market competitiveness for the insurer.

### 4.2 Targeted Preventative Health Programs

Our feature importance analysis revealed that lifestyle factors (BMI,
smoking status) are dominant predictors of high costs.

-   Implementation: Insurers can identify high-risk policyholders (e.g.,
    those in the "pre-high-cost" trajectory) and offer subsidized
    intervention programs, such as smoking cessation workshops or gym
    memberships.

-   Benefit: By proactively reducing the risk factors (lowering BMI or
    stopping smoking), insurers can significantly reduce the probability
    of catastrophic claims (e.g., cardiovascular events) in the long
    run, improving the "Loss Ratio."

### 4.3 Fraud Detection and Anomaly Flagging

The error analysis in Section 3.2.2 established a baseline for
"expected" costs.

-   Implementation: The model can serve as a benchmark engine for claims
    processing. If a claim amount significantly deviates from the
    predicted interval (specifically, if Actual Cost \>\> Predicted Cost
    without a clear medical justification like an accident), it can be
    automatically flagged for manual review.

-   Benefit: This assists in identifying potential billing errors,
    over-servicing by providers, or fraudulent claims, thereby reducing
    operational leakage.

### 4.4 Financial Reserving and Solvency

-   Implementation: On a macro level, the model can predict the
    aggregate medical expenditure for the entire portfolio of
    policyholders for the upcoming year.

-   Benefit: This allows the finance department to set aside more
    accurate cash reserves (Incurred But Not Reported - IBNR reserves),
    ensuring the company remains solvent while optimizing capital
    allocation for investments.

## 5. Conclusion

### 5.1 Summary of Findings

In this study, we developed and evaluated a series of statistical
learning models to predict medical insurance costs.

-   Exploratory Analysis: We identified that the distribution of medical
    charges is highly right-skewed and heteroscedastic. Key interaction
    effects, particularly between smoking status and BMI, were found to
    be the strongest drivers of cost escalation.

-   Model Evolution: We progressed from baseline Linear Regression (OLS)
    to regularized methods (Lasso), and finally to non-linear tree-based
    models (Random Forest and XGBoost).

    -   Linear models achieved high $R^2$ but failed to capture the
        complex structure of residuals.

    -   XGBoost utilizing the Tweedie objective function
        ($\rho \approx 1.1$) proved to be the superior model. It
        successfully handled the zero-inflated (compound Poisson-Gamma)
        nature of medical costs, minimizing the RMSE and standardizing
        the Pearson residuals.

### 5.2 Limitations: The Challenge of Aleatoric Uncertainty

Despite the high predictive accuracy ($R^2 \approx 0.98$), our residual
analysis revealed a persistent limitation. A specific cluster of
outliers—young, non-smoking, healthy individuals with skyrocketing
costs—remained difficult to predict.

-   Interpretation: These errors likely represent aleatoric
    uncertainty—random, unpredictable events such as traffic accidents
    or sudden acute injuries that are independent of demographic or
    lifestyle data.

-   Constraint: No amount of parameter tuning or model complexity can
    predict these stochastic events using the current feature set. They
    represent the inherent risk floor of the insurance business.

### 5.3 Future Directions

To further enhance model performance and business utility, we suggest
the following improvements:

1.  Integration of Longitudinal Data: Incorporating historical medical
    claims history (time-series data) rather than a snapshot would allow
    the model to detect chronic disease progression.

2.  External Data Sources: Adding geospatial data (e.g., pollution
    levels in the region) or provider data (hospital pricing tiers)
    could explain regional cost variances.

3.  Two-Stage Modeling: Developing a separate classification model to
    predict the probability of a major accident/hospitalization,
    combined with a regression model for cost severity, might better
    handle the "Black Swan" events identified in our outlier analysis.
