---
title: "Medical Insurance Cost Prediction"
author: 'SL group 6'
date: "2025-12-03"
output:
  pdf_document:
    latex_engine: xelatex
mainfont: "Microsoft YaHei"
header-includes:
  - \usepackage{xeCJK}
  - \setCJKmainfont{Microsoft YaHei}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Introduction
### 1.1 Problem Definition
### 1.2 About Dataset
This dataset provides information about 100,000 individuals including their demographics, socioeconomic status, health conditions, lifestyle factors, Insurance plans, and medical expenditures.

In detail, the columns are:

1. Demographics & Socioeconomic:
person_id, age, sex, region, urban_rural, income, education, marital_status, employment_status, household_size, dependents

2. Lifestyle & Habits:
bmi, smoker, alcohol_freq, exercise_frequency, sleep_hours, stress_level

3. Health & Clinical:
hypertension, diabetes, copd, cardiovascular, cancer_history, kidney_disease, liver_disease, arthritis, mental_health, chronic_count, systolic_bp, diastolic_bp, ldl, hba1c, risk_score, is_high_risk

4. Healthcare Utilization & Procedures:
visits_last_year, hospitalizations_last_3yrs, days_hospitalized_last_3yrs, medication_count, proc_imaging, proc_surgery, proc_psycho, proc_consult_count, proc_lab, had_major

5. Insurance & Policy:
plan_type, network_tier, deductible, copay, policy_term_years, policy_changes_last_2yrs, provider_quality

6. Medical Costs & Claims:
annual_medical_cost, annual_premium, monthly_premium, claims_count, avg_claim_amount, total_claims_paid


## 2. Exploratory Data Analysis (EDA)

- Libraries
```{r,warning=FALSE,message=FALSE}
# ============================
# Load libraries
# ============================

#install.packages("caret")
library(glmnet)
library(ggplot2)
library(dplyr)
library(tidyr)
library(ggforce)
library(gridExtra)
library(reshape2)
library(GGally)
library(corrplot)
library(e1071)
library(rpart)
library(rpart.plot)
library(tree)
library(randomForest)
library(caret)
library(xgboost)
library(Matrix)
library(caTools)
library(leaps)

```

### 2.1. Load and Inspect the dataset
```{r}
# ============================
# Load and inspect the dataset
# ============================

# Load dataset
Insurance <- read.csv("medical_insurance.csv", stringsAsFactors = FALSE)

# Quick structure and preview
str(Insurance)          # Check variable types
head(Insurance)         # Preview first few rows
summary(Insurance)      # Summary statistics

# ============================
# Check and handle missing values
# ============================

# Count missing values in each column
na_counts <- sum(is.na(Insurance))
na_counts # 0 NA data

# Count how many rows have at least one missing value
missing_rows <- sum(!complete.cases(Insurance))
missing_rows # 0 missing data

```

### 2.2. Univariate Analysis
```{r}
# ============================
# Univariate Analysis
# ============================

Insurance_view <- Insurance

# convert 0/1 columns to factors
#discrete_cols <- c("hypertension", "diabetes", "asthma", "copd", "cardiovascular_disease", "cancer_history", "kidney_disease", "liver_disease", "arthritis", "mental_health", "is_high_risk", "had_major_procedure")
discrete_cols <- c()
#Insurance_view[discrete_cols] <- lapply(Insurance_view[discrete_cols], factor)


# Convert all character columns to factors
for (v in names(Insurance_view)) {
  if (is.character(Insurance_view[[v]])) {
    Insurance_view[[v]] <- factor(Insurance_view[[v]])
  }
}


num_cols <- names(Insurance_view)[sapply(Insurance_view, is.numeric)]
cat_cols <- names(Insurance_view)[sapply(Insurance_view, is.factor)]

plot_list <- list()

# ---- Numeric Variable: Histograms ----
for (v in num_cols) {
  p <- ggplot(Insurance_view, aes(x = .data[[v]])) + 
    geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "steelblue", alpha = 0.7) +
    theme(axis.text.x = element_text(size = 4)) +

    theme_minimal()

  plot_list[[v]] <- p
}

# ---- Category Variable: Barplots ----
for (v in cat_cols) {
  p <- ggplot(Insurance_view, aes(x = .data[[v]])) + 
    geom_bar(fill = "lightgreen", color = "black") +
    geom_text(stat='count', aes(label = after_stat(count)), vjust = -0.5, size = 2) +
    theme(axis.text.x = element_text(size = 6,angle = 45, hjust = 1)) +
    theme_minimal()
  
  plot_list[[v]] <- p
}

plot_row <- 2
plot_col <- 2
plot_total <- plot_col*plot_row
num_plot <- length(plot_list)
num_pages <- floor(num_plot / plot_total)

for (page in 1:num_pages) {
  beg <- (page-1) * plot_total + 1; end <- page * plot_total
  grid.arrange(grobs = plot_list[beg:end], 
               ncol = plot_col, nrow = plot_row)
  if (page == num_pages && num_plot %% plot_total > 0) {
    grid.arrange(grobs = plot_list[(end+1):num_plot], ncol = plot_col)
  }
}




```

```{r}
# 假設您的資料集名稱為 Insurance
Insurance_view <- Insurance

# ============================
# 0. 資料預處理 (保留您的邏輯)
# ============================
# Convert all character columns to factors
for (v in names(Insurance_view)) {
  if (is.character(Insurance_view[[v]])) {
    Insurance_view[[v]] <- factor(Insurance_view[[v]])
  }
}

# ============================
# 1. Q-Q Plots (針對連續變數)
# ============================

# 1.1 BMI Q-Q Plot
p_qq_bmi <- ggplot(Insurance_view, aes(sample = bmi)) +
  stat_qq(color = "steelblue", alpha = 0.5) +
  stat_qq_line(color = "red", lwd = 1) +
  labs(title = "Q-Q Plot: BMI", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_minimal()

# 1.2 Log(Charges) Q-Q Plot
# 注意：這裡使用 log() 自然對數，若需 log10 可自行替換
p_qq_charges <- ggplot(Insurance_view, aes(sample = log1p(charges))) +
  stat_qq(color = "steelblue", alpha = 0.5) +
  stat_qq_line(color = "red", lwd = 1) +
  labs(title = "Q-Q Plot: Log(Charges)", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_minimal()

# 展示 Q-Q Plots
grid.arrange(p_qq_bmi, p_qq_charges, ncol = 2)


# ============================
# 2. Pie Charts (針對類別變數)
# ============================

# 定義一個畫圓餅圖的函數，這樣不用重複寫程式碼
draw_pie_chart <- function(data, col_name) {
  # 1. 計算數量與百分比
  plot_data <- data %>%
    count(.data[[col_name]]) %>%
    mutate(
      perc = n / sum(n),
      labels = scales::percent(perc) # 轉換成百分比格式 (e.g. 50%)
    ) %>%
    arrange(desc(perc)) # 排序讓圖表比較好看

  # 2. 繪圖
  ggplot(plot_data, aes(x = "", y = perc, fill = .data[[col_name]])) +
    geom_bar(width = 1, stat = "identity", color = "white") +
    coord_polar("y", start = 0) +
    # 加入文字標籤 (放置在扇形中間)
    geom_text(aes(label = labels), 
              position = position_stack(vjust = 0.5), 
              size = 4, color = "black") +
    labs(title = paste("Pie Chart:", col_name), x = NULL, y = NULL, fill = col_name) +
    theme_void() + # 圓餅圖通常不需要座標軸背景，使用 void 主題最乾淨
    theme(plot.title = element_text(hjust = 0.5, face = "bold")) +
    scale_fill_brewer(palette = "Pastel1") # 使用柔和的配色
}

# 繪製指定的圓餅圖
pie_sex <- draw_pie_chart(Insurance_view, "sex")
pie_smoker <- draw_pie_chart(Insurance_view, "smoker")
pie_region <- draw_pie_chart(Insurance_view, "region")

# 展示圓餅圖 (排版：上方兩個，下方一個)
grid.arrange(pie_sex, pie_smoker, pie_region, 
             layout_matrix = rbind(c(1, 2), c(3, 3)))
```

### 2.3. Bivariate Analysis

```{r}
# ============================
# Bivariate Analysis
# ============================

# ---- Feature Correlation ----


# compute correlation coefficients of "annual_medical_cost" to other features
cor_results <- sapply(c(num_cols, discrete_cols), function(v) {
  cor(Insurance[[v]], 
      Insurance$charges, use = "complete.obs")
})

cor_table <- data.frame(
  feature = names(cor_results),
  corr = as.numeric(cor_results)
)

# Sorting by "abs(corr)"
cor_table <- cor_table[order(abs(cor_table$corr), decreasing = TRUE), ]

# correlation coefficients table
cor_table


# ---- Correlation Heatmap ----
#top20_feature <- cor_table[1:20, ]$feature


corr_mat <- cor(Insurance[cor_table$feature], use = "complete.obs")

corrplot(corr_mat, method = "color",
         tl.cex = 0.5,
         tl.col = "black",
         number.cex = 0.4,
         addCoef.col = "black")

```


```{r}
Insurance_view <- Insurance

# ============================
# 1. 類別變數 vs 費用 (Categorical vs Charges)
# Use Boxplot + Jitter 看分佈與離群值
# ============================

# 定義一個繪圖函數，避免重複代碼
plot_box <- function(data, x_col, y_col, fill_color) {
  # 關鍵修正：使用 as.factor() 強制將 x 軸變數轉為類別因子
  # 這樣 scale_fill_brewer 才能運作，且 Boxplot 會正確分組
  ggplot(data, aes(x = as.factor(.data[[x_col]]), 
                   y = .data[[y_col]], 
                   fill = as.factor(.data[[x_col]]))) +
    
    geom_boxplot(alpha = 0.7, outlier.shape = NA) + 
    stat_summary(fun = mean, geom = "point", shape = 23, size = 3, fill = "white") + 
    
    # 這裡的 x 標籤我們手動設回原本的欄位名稱，不然會顯示 "as.factor(children)"
    labs(title = paste(x_col, "vs", y_col), 
         x = x_col, 
         y = y_col, 
         fill = x_col) + # 設定圖例標題
    
    theme_minimal() +
    theme(legend.position = "none") + # 通常 Boxplot 不需要圖例，因為 x 軸已經有標籤了
    scale_fill_brewer(palette = fill_color)
}

# 重新執行繪圖
p1 <- plot_box(Insurance_view, "smoker", "charges", "Set1")
p2 <- plot_box(Insurance_view, "sex", "charges", "Pastel1")
p3 <- plot_box(Insurance_view, "region", "charges", "Set2")
# 這行現在應該可以正常運作了，因為 children 會被轉為 factor
p4 <- plot_box(Insurance_view, "children", "charges", "Spectral") 

grid.arrange(p1, p2, p3, p4, ncol = 2)
```
### 2.4 Analysis of Interaction Effects

```{r}
Insurance_view$log_charges <- log1p(Insurance_view$charges)
# ============================
# 1. BMI * Smoker 
# ============================

p_bmi_smoker <- ggplot(Insurance_view, aes(x = bmi, y = charges, color = smoker)) +
  geom_point(alpha = 0.5, size = 1) +
  geom_smooth(method = "lm", se = FALSE, size = 1.2) + # 加粗回歸線
  scale_color_manual(values = c("yes" = "#c0392b", "no" = "#2980b9")) +
  labs(title = "1. Interaction: BMI * Smoker",
       x = "BMI", y = "Charges") +
  theme_minimal() +
  theme(legend.position = "top")


p_bmi_smoker
```

```{r}
# ============================
# 1. BMI * Smoker 
# ============================
log_p_bmi_smoker <- ggplot(Insurance_view, aes(x = bmi, y = log_charges, color = smoker)) +
  geom_point(alpha = 0.5, size = 1) +
  geom_smooth(method = "lm", se = FALSE, size = 1.2) + # 加粗回歸線
  scale_color_manual(values = c("yes" = "#c0392b", "no" = "#2980b9")) +
  labs(title = "1. Interaction: BMI * Smoker",
       x = "BMI", y = "Log1p(Charges)") +
  theme_minimal() +
  theme(legend.position = "top")
log_p_bmi_smoker
```

```{r}
# ============================
# 2. Age * Smoker 
# ============================
p_age_smoker <- ggplot(Insurance_view, aes(x = age, y = charges, color = smoker)) +
  geom_point(alpha = 0.5, size = 1) +
  geom_smooth(method = "lm", se = FALSE, size = 1.2) +
  scale_color_manual(values = c("yes" = "#c0392b", "no" = "#2980b9")) +
  labs(title = "2. Interaction: Age * Smoker",
       x = "Age", y = "Charges") +
  theme_minimal() +
  theme(legend.position = "top")

p_age_smoker
```

```{r}
# ============================
# 2. Age * Smoker 
# ============================
log_p_age_smoker <- ggplot(Insurance_view, aes(x = age, y = log_charges, color = smoker)) +
  geom_point(alpha = 0.5, size = 1) +
  geom_smooth(method = "lm", se = FALSE, size = 1.2) +
  scale_color_manual(values = c("yes" = "#c0392b", "no" = "#2980b9")) +
  labs(title = "2. Interaction: Age * Smoker",
       x = "Age", y = "Log1p(Charges)") +
  theme_minimal() +
  theme(legend.position = "top")

log_p_age_smoker
```

```{r}
# ============================
# 特殊處理：為了畫 bmi:age，我們需要把 age 分組
# ============================
# 將年齡切分為三個階段：青年(18-35), 中年(36-55), 老年(55+)
Insurance_view <- Insurance_view %>%
  mutate(age_group = cut(age, 
                         breaks = c(0, 35, 55, 100), 
                         labels = c("Young (18-35)", "Middle (36-55)", "Senior (55+)")))
# ============================
# 3. BMI * Age 
# ============================
# 這裡使用我們剛才創造的 age_group 來分組
p_bmi_age <- ggplot(Insurance_view, aes(x = bmi, y = charges, color = age_group)) +
  geom_point(alpha = 0.3, size = 1) +
  geom_smooth(method = "lm", se = FALSE, size = 1.2) +
  scale_color_brewer(palette = "Dark2") + # 使用高對比顏色
  labs(title = "3. Interaction: BMI * Age (Grouped)",
       x = "BMI", y = "Charges", color = "Age Group") +
  theme_minimal() +
  theme(legend.position = "top")

p_bmi_age
```

```{r}
# ============================
# 3. BMI * Age 
# ============================
# 這裡使用我們剛才創造的 age_group 來分組
p_bmi_age <- ggplot(Insurance_view, aes(x = bmi, y = log_charges, color = age_group)) +
  geom_point(alpha = 0.3, size = 1) +
  geom_smooth(method = "lm", se = FALSE, size = 1.2) +
  scale_color_brewer(palette = "Dark2") + # 使用高對比顏色
  labs(title = "3. Interaction: BMI * Age (Grouped)",
       x = "BMI", y = "Log1p(Charges)", color = "Age Group") +
  theme_minimal() +
  theme(legend.position = "top")

p_bmi_age
```


## 3. Method

- Data Preprocessing
```{r}

# ============================
# Data Preprocessing
# ============================


# data frame with all avaliable feature
df_raw <- Insurance

# Convert all character columns to factors
for (v in names(df_raw)) {
  if (is.character(df_raw[[v]])) {
    df_raw[[v]] <- factor(df_raw[[v]])
  }
}


# convert 0/1 columns to factors
discrete_cols <- c()

non_char_cols <- names(df_raw)[sapply(df_raw, is.numeric)]
num_cols <- setdiff(non_char_cols, discrete_cols)

# ---- One-hot encoding (dummy variables) ----
 # This turns each factor into 0/1 columns
df_ori <- model.matrix(~ ., data = df_raw)[,-1]

# Final cleaned numeric dataset
df_ori <- as.data.frame(df_ori)
# convert col names to valid names
names(df_ori) <- make.names(names(df_ori))



# View origininal dataset
str(df_ori)
head(df_ori)

# ---- Data Transformation ----
df_trans <- df_ori

for(v in num_cols) {
  if(skewness(df_trans[,v]) > 1) {
    # log1p transform
    df_trans[,v] <- log1p(df_trans[,v])
    
    before_trans <- ggplot(df_ori, aes(x = .data[[v]])) + 
    geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "steelblue", alpha = 0.7) +
    theme(axis.text.x = element_text(size = 4)) +
    theme_minimal() + 
    labs(
      title = paste("Before Transformation:", v)
    )
    after_trans <- ggplot(df_trans, aes(x = .data[[v]])) + 
    geom_histogram(aes(y = after_stat(density)), bins = 30, 
                   fill = "steelblue", alpha = 0.7) +
    theme(axis.text.x = element_text(size = 4)) +
    theme_minimal() +
    labs(
      title = paste("After Transformation:", v),
      x = paste0("log1p(", v, ")")      # x label 直接寫清楚
    )
    grid.arrange(grobs = list(before_trans, after_trans), ncol = 2)

  }
}



```
We first apply one-hot encoding to all categorical variables. For right-skewed numerical variables, we apply a log1p transformation.


- Data Splitting
```{r split-data}

set.seed(123)

# Number of observations
n <- nrow(df_trans)

# 80% for training, 20% for test
train_idx <- sample(seq_len(n), size = 0.8 * n)

train <- df_trans[train_idx, ]
test  <- df_trans[-train_idx, ]

train_ori <- df_ori[train_idx, ]
test_ori  <- df_ori[-train_idx, ]
```

We randomly split the cleaned dataset into a 80% training set and a 20% test set using a fixed random seed to ensure reproducibility. All models in this section are trained on the training set and evaluated on the test set for fair comparison.

- Some Useful Function
```{r}
# RMSE
computeRMSE <- function(y_pred, y_true, transform = FALSE) {
  if (transform == TRUE) {
    y_pred <- expm1(y_pred)
    y_true <- expm1(y_true)
  }
  rmse <- sqrt(mean((y_pred - y_true)^2))
    
  return(rmse)
}

# R^2
computeR2 <- function(y_pred, y_true, transform = FALSE) {
  if (transform == TRUE) {
    y_pred <- expm1(y_pred)
    y_true <- expm1(y_true)
  }
  
  SSE <- sum((y_true - y_pred)^2)
  SST <- sum((y_true - mean(y_true))^2)
  
  R2 <- 1 - SSE/SST
  
  return(R2)
}

```

### 3.1 Linear Regression Model (Parametric)

#### 3.1.1 Multiple Linear Regression Model(Baseline)


```{r baseline-ols}
# Response variable
#response_var <- "annual_medical_cost"
response_var <- "charges"
# Variables that would leak cost information — must be excluded
# sus_vars <- c("annual_premium", "monthly_premium", "claims_count", "avg_claim_amount", "total_claims_paid")
leakage_vars <- c()


# Construct predictor set
predictor_vars <- setdiff(
  names(df_trans),
  c(response_var, leakage_vars)
)



# Build model formula
form_baseline <- as.formula(
  paste(response_var, "~", paste(predictor_vars, collapse = "+"))
)

# Fit OLS
lm_full <- lm(form_baseline, data = train)
summary(lm_full)

# Predictions
train_pred_lm <- predict(lm_full, newdata = train)
test_pred_lm  <- predict(lm_full, newdata = test)

# Compute R^2
y_train <- train[[response_var]]
y_test  <- test[[response_var]]

R2_train_lm <- computeR2(train_pred_lm, y_train, TRUE)
#  1 - sum((y_train - train_pred_lm)^2) / sum((y_train - mean(y_train))^2)

R2_test_lm  <- computeR2(test_pred_lm, y_test, TRUE)
# 1 - sum((y_test - test_pred_lm)^2) / sum((y_test - mean(y_test))^2)

# Compute RMSE
RMSE_train_lm <- computeRMSE(train_pred_lm, y_train, TRUE)
RMSE_test_lm  <- computeRMSE(test_pred_lm, y_test, TRUE)

R2_train_lm; R2_test_lm
RMSE_train_lm; RMSE_test_lm
```

We first fit a multiple linear regression model (OLS) using annual_medical_cost as the response variable. All demographic, lifestyle, health, and insurance-related variables were included as predictors, except for other cost-related fields such as annual_premium, monthly_premium, and total_claims_paid to avoid information leakage.

The baseline OLS model achieves a training R^2 of 0.9904 and a test R^2 of 0.9898, with corresponding RMSE values of 0.0363 (train) and 0.0371 (test).

These results indicate that the linear specification explains the vast majority of variance in annual medical cost and generalizes well to the test set, suggesting minimal overfitting.

The baseline OLS model therefore provides a strong and interpretable reference point for evaluating the effects of Lasso regularization in the subsequent sections.


#### 3.1.2 Lasso Linear Regression Model with CV
```{r lasso-cv-setup}

# Construct design matrices (remove intercept)
x_train_base <- model.matrix(form_baseline, data = train)[, -1]
x_test_base  <- model.matrix(form_baseline, data = test)[, -1]
head(x_train_base)
head(x_test_base)
```

```{r lasso-cv-fit}
set.seed(123)

cv_lasso_base <- cv.glmnet(
  x = x_train_base,
  y = y_train,
  alpha = 1,        # Lasso
  nfolds = 10,
  standardize = TRUE
)

lambda_min_base <- cv_lasso_base$lambda.min

cat("Best lambda: ", lambda_min_base, '\n')

coef(cv_lasso_base, s = lambda_min_base)

```

To address potential multicollinearity and reduce the dimensionality of the baseline linear regression model, we apply Lasso (L1) regularization to the same set of predictors. Lasso shrinks coefficient estimates toward zero and can effectively eliminate weak predictors, resulting in a more stable and interpretable model.

We use 10-fold cross-validation to select the optimal regularization parameter $\lambda$ . The value that minimizes the cross-validated mean squared error is:

$\lambda_{\text{min}}$ = 0.000359.

This $\lambda$ value represents the level of shrinkage that provides the best out-of-sample predictive performance for the Lasso model.

```{r lasso-cv-performance}
# Predictions for CV-selected lambda.min
train_pred_cv_base <- predict(cv_lasso_base, newx = x_train_base, s = "lambda.min")
test_pred_cv_base  <- predict(cv_lasso_base, newx = x_test_base,  s = "lambda.min")

# Compute R^2
R2_train_cv_base <- computeR2(y_train, train_pred_cv_base, TRUE)
#  1 - sum((y_train - train_pred_cv_base)^2) / sum((y_train - mean(y_train))^2)

R2_test_cv_base  <- computeR2(y_test, test_pred_cv_base, TRUE)
#  1 - sum((y_test - test_pred_cv_base)^2) / sum((y_test - mean(y_test))^2)

# Compute RMSE
RMSE_train_cv_base <- computeRMSE(y_train, train_pred_cv_base, TRUE)
RMSE_test_cv_base  <- computeRMSE(y_test, test_pred_cv_base, TRUE)

R2_train_cv_base; R2_test_cv_base
RMSE_train_cv_base; RMSE_test_cv_base
```

Using the optimal penalty parameter selected via 10-fold cross-validation, the Lasso model achieves strong predictive performance. The training and testing results are summarized as follows:

	•	Training $R^2$ = 0.9415
	
	•	Test $R^2$ = 0.9400
	
	•	Training RMSE = 0.0896
	
	•	Test RMSE = 0.0900

These results show that the CV-selected Lasso model performs nearly identically to the baseline OLS model in terms of predictive accuracy. However, unlike OLS, Lasso shrinks several coefficients toward zero, effectively reducing model complexity and improving interpretability without sacrificing test-set performance. This makes the CV-based Lasso a more stable and parsimonious alternative to the unregularized linear model.

#### 3.1.3 Lasso Regression Model with BIC
```{r fit full Lasso path & compute BIC}
# Fit full Lasso path on baseline features
lasso_base <- glmnet(
  x = x_train_base,
  y = y_train,
  alpha = 1,
  standardize = TRUE
)


# Function to compute BIC for each lambda
calc_bic <- function(fit, x, y) {
  n <- length(y)
  y_hat <- predict(fit, newx = x)
  rss <- colSums((y - y_hat)^2)
  df  <- fit$df  # number of non-zero coefficients
  bic <- n * log(rss / n) + df * log(n)
  
  return(bic)
}

bic_base <- calc_bic(lasso_base, x_train_base, y_train)

lambda_bic_base <- lasso_base$lambda[which.min(bic_base)]

cat("Best lambda: ", lambda_bic_base, '\n')

coef(lasso_base, s = lambda_bic_base)


plot(log(lasso_base$lambda), bic_base, type = "l",
     xlab = "log(lambda)", ylab = "BIC")
abline(v = log(lambda_bic_base), lty = 2)
```



While cross-validation selects the penalty parameter that minimizes prediction error, it often results in a relatively flexible model with more nonzero coefficients. To obtain a more parsimonious model, we additionally apply the Bayesian Information Criterion (BIC) to select the optimal value of $\lambda$ along the Lasso regularization path.

We compute the BIC for each candidate $\lambda$ by combining model fit and model complexity. As shown in the BIC curve, the minimum value occurs at:

$\lambda_{\text{BIC}}$ = 0.0004747.

This value is larger than the CV-selected $\lambda_{\min}$ , indicating stronger shrinkage and a more aggressive reduction of coefficients. The vertical dashed line in the BIC plot marks the location of the selected $\lambda$ , which corresponds to the most parsimonious model under the BIC criterion.


```{r use BIC λ to evaluate performance}
# Predictions using BIC-selected lambda
train_pred_bic_base <- predict(lasso_base, newx = x_train_base, s = lambda_bic_base)
test_pred_bic_base  <- predict(lasso_base, newx = x_test_base,  s = lambda_bic_base)

# Compute R^2
R2_train_bic_base <- computeR2(y_train, train_pred_bic_base, TRUE)
# 1 - sum((y_train - train_pred_bic_base)^2) / sum((y_train - mean(y_train))^2)

R2_test_bic_base  <- computeR2(y_test, test_pred_bic_base, TRUE)
# 1 - sum((y_test - test_pred_bic_base)^2) / sum((y_test - mean(y_test))^2)

# Compute RMSE
RMSE_train_bic_base <- computeRMSE(y_train, train_pred_bic_base, TRUE)
RMSE_test_bic_base  <- computeRMSE(y_test, test_pred_bic_base, TRUE)

R2_train_bic_base; R2_test_bic_base
RMSE_train_bic_base; RMSE_test_bic_base
```

Using the penalty parameter selected by BIC, the Lasso model achieves the following predictive performance:

	•	Training $R^2$ = 0.94147
	
	•	Test $R^2$ = 0.94002
	
	•	Training RMSE = 0.08960
	
	•	Test RMSE = 0.08998

The predictive accuracy of the BIC-selected model is nearly identical to that of the CV-selected Lasso model. But BIC applies a stronger penalty and favors simpler models, it leads to a sparser coefficient structure with more variables shrunk toward zero. This results in a more parsimonious model while maintaining essentially the same out-of-sample accuracy.


```{r baseline-summary}


results_baseline <- data.frame(
  Model = c("OLS baseline",
            "Lasso (CV λ_min)",
            "Lasso (BIC)"),
  R2_test = c(R2_test_lm, R2_test_cv_base, R2_test_bic_base),
  RMSE_test = c(RMSE_test_lm, RMSE_test_cv_base, RMSE_test_bic_base)
)

results_baseline
```

Table reports the test performance of the three baseline linear models.
The OLS model shows the highest accuracy (test R^2 = 0.9898, RMSE = 0.0371) and explains most of the variation in medical cost. OLS keeps all predictors and applies no coefficient shrinkage.

The two Lasso models—CV and BIC—yield lower accuracy (test $R^2$ $\approx$ 0.94, RMSE ≈ 0.09), but produce more compact models. Lasso shrinks many coefficients toward zero and reduces model complexity. The CV-selected Lasso retains more predictors; the BIC-selected Lasso produces the sparsest structure with similar predictive performance.

Overall, OLS provides the best predictive accuracy, while Lasso offers simpler and more interpretable models, serving as useful baselines for comparison with nonlinear approaches in later sections.

```{r}

# --- OLS (baseline) ---
results_ols <- data.frame(preds = expm1(test_pred_lm), actual = expm1(y_test))
results_ols$residuals <- results_ols$actual - results_ols$preds

# Prediction vs Actual
PvA_ols <- ggplot(results_ols, aes(x = preds, y = actual)) +
  geom_point(alpha = 0.5) +
  geom_abline(color = "red") +
  theme_minimal() +
  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+

  labs(title = "OLS Prediction vs Actual")

# Prediction vs Residuals
PvR_ols <- ggplot(results_ols, aes(x = preds, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red") +
  theme_minimal() +
  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  labs(title = "OLS Prediction vs Residuals")

grid.arrange(grobs = list(PvA_ols, PvR_ols), ncol = 2)
```

1.

The OLS prediction–actual plot shows points concentrated tightly along the 45° line, indicating strong linear fit. The residual plot reveals a clear curved pattern, suggesting non-linearity not captured by the model. Residuals deviate systematically at both low and high predicted values, indicating model misspecification.



```{r}
# --- CV ---

test_pred_cv_base <- as.numeric(test_pred_cv_base)

results_cv <- data.frame(preds = expm1(test_pred_cv_base), actual = expm1(y_test))
results_cv$residuals <- results_cv$actual - results_cv$preds

# Prediction vs Actual
PvA_cv <- ggplot(results_cv, aes(x = preds, y = actual)) +
  geom_point(alpha = 0.5) +
  geom_abline(color = "red") +
  theme_minimal() +
  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+

  labs(title = "Best CV Prediction vs Actual")

# Prediction vs Residuals
PvR_cv <- ggplot(results_cv, aes(x = preds, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red") +
  theme_minimal() +
  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  labs(title = "Best CV Prediction vs Residuals")

grid.arrange(grobs = list(PvA_cv, PvR_cv), ncol = 2)
```

2.

The CV-selected Lasso model produces a prediction–actual relationship similar to OLS, though slightlymore compressed due to coefficient shrinkage. The residual plot also shows a pronounced curvedpattern, indicating remaining non-linearity. Shrinkage improves model stability but does not resolve the structural deviation visible in residuals.

```{r}
# --- BIC ---
test_pred_bic_base <- as.numeric(test_pred_bic_base)

results_bic <- data.frame(preds = expm1(test_pred_bic_base), actual = expm1(y_test))
results_bic$residuals <- results_bic$actual - results_bic$preds

# Prediction vs Actual
PvA_bic <- ggplot(results_bic, aes(x = preds, y = actual)) +
  geom_point(alpha = 0.5) +
  geom_abline(color = "red") +
  theme_minimal() +
  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  labs(title = "Best BIC Prediction vs Actual")

# Prediction vs Residuals
PvR_bic <- ggplot(results_bic, aes(x = preds, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red") +
  theme_minimal() +
  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  labs(title = "Best BIC Prediction vs Residuals")
grid.arrange(grobs = list(PvA_bic, PvR_bic), ncol = 2)

```

3.

The BIC-selected Lasso model shows nearly identical prediction–actual behavior as the CV model, with even stronger shrinkage leading to a slightly more compressed fit. The residual plot again displays the same curved structure, confirming that model simplification does not address the underlying non-linearity in the data.

Summary

Across OLS, CV-Lasso, and BIC-Lasso, prediction accuracy remains high, but all residual plots reveal systematic curvature. This indicates that the baseline linear structure is insufficient and motivates the use of nonlinear terms or more flexible models in later sections.

#### 3.1.4 

### subset selection

```{r}
train_set = train[,]
test_set = test[,]

str(train)
str(train_set)

```

```{r}
set.seed(123)
regfit.full = regsubsets(charges ~ ., data = train_set)
reg1.summary = summary(regfit.full)
reg1.summary
```

```{r}
par(mfrow=c(1,2))
index = which.max(reg1.summary$adjr2)
cat("\nSize:", index, "\n")
cat("Adjusted RSq:", reg1.summary$adjr2[index],"\n")
coef(regfit.full,index)
plot(reg1.summary$adjr2, xlab='Number of Variables',ylab='Adjusted RSq',type='b', pch=19)
points(index,reg1.summary$adjr2[index],col = "blue", pch=19)


index = which.min(reg1.summary$bic)
cat("\nSize:", index, "\n")
cat("BIC:", reg1.summary$bic[index],"\n")
coef(regfit.full,index)
plot(reg1.summary$bic, xlab='Number of Variables',ylab='BIC',type='b', pch=19)
points(index,reg1.summary$bic[index],col = "blue", pch=19)

```

```{r}
par(mfrow=c(1,2))

test.mat = model.matrix(charges~.,data = test_set)
test.errors = rep(NA,8)
for(i in 1:8){
  coefi = coef(regfit.full,id=i)
  intercept = as.vector(coefi)[1]
  pred = test.mat[,names(coefi)] %*% coefi
  test.errors[i] = sqrt(mean((test_set$charges - pred)^2))
}
min.index = which.min(reg1.summary$rss)
cat("\nSize:", min.index, "\n")
cat("Train RMSE:", sqrt(reg1.summary$rss[min.index]/n),"\n")
coef(regfit.full,id=min.index)
plot(sqrt((reg1.summary$rss)/n), xlab='Number of Variables',ylab='Train RMSE',type='b',pch=19)
points(min.index, sqrt(reg1.summary$rss[min.index]/n),col = "blue", pch=19)

min.index = which.min(test.errors)
cat("\nSize:", min.index, "\n")
cat("Test RMSE:", test.errors[min.index],"\n")
coef(regfit.full,id=min.index)
plot(1:8, test.errors,xlab='Number of variables', ylab='Test RMSE',type="b", pch=19)
points(min.index, test.errors[min.index],col = "blue", pch=19)
```

#### forward selection

```{r}
set.seed(123)
regfit.fwd = regsubsets(charges~.,data = train_set,method = 'forward')
reg.fwd.summary = summary(regfit.fwd)
reg.fwd.summary
```

```{r}
par(mfrow=c(1,2))

index = which.max(reg.fwd.summary$adjr2)
cat("\nSize:", index, "\n")
cat("Adjusted RSq:", reg.fwd.summary$adjr2[index],"\n")
coef(regfit.fwd,index)
plot(reg.fwd.summary$adjr2, xlab='Number of Variables',ylab='Adjusted RSq',type='b', pch=19)
points(index,reg.fwd.summary$adjr2[index],col = "blue", pch=19)


index = which.min(reg.fwd.summary$bic)
cat("\nSize:", index, "\n")
cat("BIC:", reg.fwd.summary$bic[index],"\n")
coef(regfit.fwd,index)
plot(reg.fwd.summary$bic, xlab='Number of Variables',ylab='BIC',type='b', pch=19)
points(index,reg.fwd.summary$bic[index],col = "blue", pch=19)
```

```{r}
par(mfrow=c(1,2))

test.mat = model.matrix(charges~.,data = test_set)
test.errors = rep(NA,8)
for(i in 1:8){
  coefi = coef(regfit.fwd,id=i)
  intercept = as.vector(coefi)[1]
  pred = test.mat[,names(coefi)] %*% coefi
  test.errors[i] = sqrt(mean((test_set$charges - pred)^2))
}
min.index = which.min(reg.fwd.summary$rss)
cat("\nSize:", min.index, "\n")
cat("Train RMSE:", sqrt(reg.fwd.summary$rss[min.index]/n),"\n")
coef(regfit.fwd,id=min.index)
plot(sqrt((reg.fwd.summary$rss)/n), xlab='Number of Variables',ylab='Train RMSE',type='b',pch=19)
points(min.index, sqrt(reg.fwd.summary$rss[min.index]/n),col = "blue", pch=19)

min.index = which.min(test.errors)
cat("\nSize:", min.index, "\n")
cat("Test RMSE:", test.errors[min.index],"\n")
coef(regfit.fwd,id=min.index)
plot(1:8, test.errors,xlab='Number of variables', ylab='Test RMSE',type="b", pch=19)
points(min.index, test.errors[min.index],col = "blue", pch=19)
```

#### backward selection

```{r}
set.seed(123)
regfit.bwd = regsubsets(charges~.,data = train_set,method = 'backward')
reg.bwd.summary = summary(regfit.bwd)
reg.bwd.summary
```

```{r}
par(mfrow=c(1,2))

index = which.max(reg.bwd.summary$adjr2)
cat("\nSize:", index, "\n")
cat("Adjusted RSq:", reg.bwd.summary$adjr2[index],"\n")
coef(regfit.bwd,index)
plot(reg.bwd.summary$adjr2, xlab='Number of Variables',ylab='Adjusted RSq',type='b', pch=19)
points(index,reg.bwd.summary$adjr2[index],col = "blue", pch=19)


index = which.min(reg.bwd.summary$bic)
cat("\nSize:", index, "\n")
cat("BIC:", reg.bwd.summary$bic[index],"\n")
coef(regfit.bwd,index)
plot(reg.bwd.summary$bic, xlab='Number of Variables',ylab='BIC',type='b', pch=19)
points(index,reg.bwd.summary$bic[index],col = "blue", pch=19)
```

```{r}
par(mfrow = c(1, 2))
test.mat = model.matrix(charges~.,data = test_set)
test.errors = rep(NA,8)
for(i in 1:8){
  coefi = coef(regfit.bwd,id=i)
  intercept = as.vector(coefi)[1]
  pred = test.mat[,names(coefi)] %*% coefi
  test.errors[i] = sqrt(mean((test_set$charges - pred)^2))
}
min.index = which.min(reg.bwd.summary$rss)
cat("\nSize:", min.index, "\n")
cat("Train RMSE:", sqrt(reg.bwd.summary$rss[min.index]/n),"\n")
coef(regfit.bwd,id=min.index)
plot(sqrt((reg.bwd.summary$rss)/n), xlab='Number of Variables',ylab='Train RMSE',type='b',pch=19)
points(min.index, sqrt(reg.bwd.summary$rss[min.index]/n),col = "blue", pch=19)

min.index = which.min(test.errors)
cat("\nSize:", min.index, "\n")
cat("Test RMSE:", test.errors[min.index],"\n")
coef(regfit.bwd,id=min.index)
plot(1:8, test.errors,xlab='Number of variables', ylab='Test RMSE',type="b", pch=19)
points(min.index, test.errors[min.index],col = "blue", pch=19)
```
#### result
- Linear regression model with 7 predictors by feature selection:
```{r}
predictors = setdiff(
  names(train),
  c('charges','regionnorthwest')
)
form_fs <- as.formula(
  paste('charges', "~", paste(predictors, collapse = "+"))
)
lm_fs <- lm(form_fs,data = train)
summary(lm_fs)

# Predictions
train_pred_fs <- predict(lm_fs, newdata = train)
test_pred_fs  <- predict(lm_fs, newdata = test)

# Compute R^2
y_train <- train$charges
y_test  <- test$charges

R2_train <- computeR2(train_pred_fs, y_train, TRUE)
#  1 - sum((y_train - train_pred)^2) / sum((y_train - mean(y_train))^2)
R2_test  <- computeR2(test_pred_fs, y_test, TRUE) 
#  1 - sum((y_test - test_pred_fs)^2) / sum((y_test - mean(y_test))^2)

# Compute RMSE
RMSE_train <- computeRMSE(train_pred_fs, y_train, TRUE)
RMSE_test  <- computeRMSE(test_pred_fs, y_test, TRUE)

cat('Train R^2:',R2_train,'\n')
cat('Test R^2:',R2_test,'\n')
cat('Train RMSE:',RMSE_train,'\n')
cat('Test RMSE:',RMSE_test,'\n')
```

```{r}
results <- data.frame(preds = expm1(test_pred_fs), actual = expm1(test_set$charges) )
results$residuals <- results$actual - results$preds

# Prediction vs Actual
PvA_fs <- ggplot(results, aes(x = preds, y = actual)) +
  geom_point(alpha = 0.5) +
  geom_abline(color = "red") +
  theme_minimal() +
  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  labs(title = "Prediction vs Actual")

# Prediction vs Residuals
PvR_fs <- ggplot(results, aes(x = preds, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red") +
  theme_minimal() +
  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  labs(title = "Prediction vs Residuals")

grid.arrange(grobs = list(PvA_fs, PvR_fs), ncol = 2)

```

### 3.2 Tree-based Model (Non-parametric)
  Tree-based models are more flexible non-parametric methods that don't assume a specific functional form between predictors and response. They can effectively capture the non-linear and complex relationships in the datasets.
  In this section, we want apply non-parametric models to compare with the parametric models in the previous section. In particular, we train random forest and XGBoost models, representing  aggregation and boosting methods, respectively. We also compare their performance of these two models.

- Data Setup
```{r}
# --- Variable Setup ---
# response variable (target)
response_var <- c("charges")

# Predictor variable set
predictor_vars <- setdiff(
  names(df_ori),
  c(response_var)
)

y_train <- train_ori[,response_var]
y_test <- test_ori[,response_var]

```

#### 3.2.1 Random Forest

```{r}

# ==========================================
#  Random Forest model with CV
# ==========================================

set.seed(321)

# --- Parameter Setup ---

# Construct model formula
form_rf <- as.formula(
  paste(response_var, "~", paste(predictor_vars, collapse = "+"))
)

# --- Training model ---

# number of tree (ntree)
B <- 200
# number of predictors
p <- ncol(train_ori) - 1

# rf <- randomForest(form_rf, data = train_raw, 
#                       mtry = p/2, ntree = B, importance = TRUE)

# 5-fold Cross-Validation
cv_ctrl <- trainControl(method = "cv", number = 5)

# --- training random forest with CV ---
rf_cv <- train(
  form_rf, 
  data = train_ori,
  method = "rf",
  trControl = cv_ctrl,
  tuneLength = p-1, # try mtry = 1 to (p-1)
  ntree = B, # B trees
)

# The result of best random forest
rf_cv

# Variable Importance
varImp(rf_cv)

```
The best number of features for the random forest is 5, which has the highest $R^2$ and the lowest RMSE.

```{r}

# ==========================================
#  Computing metrics
# ==========================================

# Predictions
rf_pred_train <- predict(rf_cv, newdata = train_ori)
rf_pred_test <- predict(rf_cv, newdata = test_ori)

# Train RMSE & R2
rf_train_rmse <- computeRMSE(rf_pred_train, y_train)
rf_train_R2 <- computeR2(rf_pred_train, y_train)

# Test RMSE & R2 (the most important metric)
rf_test_rmse <- computeRMSE(rf_pred_test, y_test)
rf_test_R2 <- computeR2(rf_pred_test, y_test)

# Output the result
cat("Training RMSE:", rf_train_rmse, "\n")
cat("Testing RMSE: ", rf_test_rmse, "\n")
cat("Training R2:  ", rf_train_R2, "\n")
cat("Testing R2:   ", rf_test_R2, "\n")


```



```{r}

# ==========================================
#  Prediction and Residual Plot
# ==========================================



# Build plot DataFrame
plot_data <- data.frame(
  Predicted = rf_pred_test,
  #  Residuals = actual - predicts
  actual = y_test,
  Residuals = y_test - rf_pred_test
)

# --- plots ---
# 1. actual vs predicts (Ideally, the points should follow 45-degree line.)
PvA_rf <- ggplot(plot_data, aes(x = Predicted, y = actual)) +
  geom_point(alpha = 0.5, color = "darkblue") + # scatter point
  geom_abline(color = "red", linetype = "dashed") + # 45-degree
  labs(title = "Prediction vs Actual", 
    x = "Predicted Charges",
    y = "True Charges") +
  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  theme_minimal()

# 2. Residual Distribution (The points should follow horizontal line.)
PvR_rf <- ggplot(plot_data, aes(x = Predicted, y = Residuals)) +
  geom_point(alpha = 0.5, color = "darkblue") + # scatter point
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") + # horizontal
  theme_minimal() +
  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  labs(
    title = "Residual Plot (Random Forest)",
    x = "Predicted Charges",
    y = "Residuals (Actual - Predicted)"
  )

grid.arrange(grobs = list(PvA_rf, PvR_rf), ncol = 2)
```

#### 3.2.2 XGBoost

```{r}
# ==========================================
#  XGBoost with CV
# ==========================================

dtrain <- xgb.DMatrix(data = as.matrix(train_ori %>% select(-charges)), label = train_ori$charges)
params <- list(
  booster = "gbtree",
  objective = "reg:tweedie", # Importance! This is the target function for regression
  eval_metric = "rmse",       # evaluate model by RMSE
  eta = 0.05,                 # Learning rate
  max_depth = 6,              # max depth of each tree
  subsample = 0.8,            # Row subsampling rate
  colsample_bytree = 1,       # Feature subsampling rate
  gamma = 0.05,                   
  min_child_weight = 5        # Min sum of instance weight needed in a child node
)

# --- Training Model ---

# Using CV to find best rounds (nrounds) and avoid overfitting
cv_model <- xgb.cv(
  set.seed(42),
  params = params,
  data = dtrain,
  nrounds = 1000,        # Upper bound for number of boosting rounds
  nfold = 5,             # 5-fold cross validation
  early_stopping_rounds = 10, # Stop if no improvement for 10 rounds
  verbose = 1,           # Display training progress
  print_every_n = 100
)

# Extract the best number of boosting rounds

best_nrounds <- cv_model$best_iteration


# Train the final XGBoost model with optimal parameters
final_model <- xgb.train(
  set.seed(42),
  params = params,
  data = dtrain,
  nrounds = best_nrounds
)
```

```{r}
dtest <- xgb.DMatrix(data = as.matrix(test_ori %>% select(-charges)), label = test_ori$charges)
pred <- predict(final_model, dtest)

# --- Accuracy Validation ---
# Compute RMSE
rmse <- computeRMSE(pred, y_test)

cat("--- Model Evaluation ---\n")
cat("Testing RMSE (USD):", rmse, "\n")

# Compute R-squared
r_sq <- computeR2(pred, y_test)
cat("Testing R-squared:", r_sq, "\n")
```

```{r}
results <- data.frame(preds = pred, actual = test_ori$charges)
results$residuals <- results$actual - results$preds

# 1. actual vs predicts (The points should follow 45-degree line.)
PvA_xgb <- ggplot(results, aes(x = preds, y = actual)) +
  geom_point(alpha = 0.5) +
  geom_abline(color = "red") +
  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  theme_minimal() +
  labs(title = "Prediction vs Actual")

# 2. Residual Distribution (The points should follow horizontal line.)
PvR_xgb <- ggplot(results, aes(x = preds, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red") +
  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  theme_minimal() +
  labs(title = "Prediction vs Residual")

grid.arrange(grobs = list(PvA_xgb, PvR_xgb), ncol = 2)

```


```{r}
# 1. Compute importance matrix
importance_matrix <- xgb.importance(feature_names = colnames(dtrain), model = final_model)

# 2. Watch parameters
print(importance_matrix)

# 3. Draw plot
xgb.plot.importance(importance_matrix)
```
