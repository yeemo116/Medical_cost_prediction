---
title: "Medical Insurance Cost Prediction"
author: 'SL group 6'
date: "2025-12-03"
output:
  pdf_document:
    latex_engine: xelatex
mainfont: "Microsoft YaHei"
header-includes:
  - \usepackage{xeCJK}
  - \setCJKmainfont{Microsoft YaHei}
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Introduction

Medical insurance cost prediction is an important problem in the field
of health analytics, where insurers aim to estimate the healthcare
expenses of individuals based on demographic, lifestyle, and behavioral
factors. Accurate prediction models allow insurance companies to design
fair pricing strategies, assess risk, and optimize profitability.

By leveraging demographic information such as age and sex, lifestyle
indicators such as BMI and smoking habits, and contextual factors such
as region, models can provide valuable insights into the drivers of
medical expenses. This study uses a publicly available medical insurance
dataset to compare several regression approaches and evaluate their
predictive performance.

In the following sections, we conduct exploratory data analysis, build
multiple predictive models, and compare their performance to identify
the most effective approach for estimating medical insurance costs.

### 1.1 Motivation

We chose this topic because medical insurance is something that affects
everyone, yet the factors that determine medical costs are not always
obvious. As students, we wanted to explore how personal characteristics
such as age, BMI, and smoking habits translate into differences in
expected medical expenses.

By studying this dataset, we can better understand a real issue that
people encounter in daily life, while also applying statistical learning
methods to a practical and relatable problem.

### 1.2 Problem Definition

The primary objective of this study is to predict an individual’s
medical insurance cost based on demographic and lifestyle
characteristics. Specifically, we aim to address the following
questions:

#### 1. What are the main factors influencing medical expenses?

Understanding these factors helps identify risk contributors and
supports evidence-based pricing decisions.

#### 2. How accurately can statistical learning models predict medical expenses?

We evaluate multiple regression models—including linear regression,
regularized models, and non-linear models—to determine their predictive
performance.

#### 3. How can predictive modeling enhance efficiency and profitability for insurance companies?

Reliable models can improve premium calculations, risk stratification,
and long-term financial planning.

Through these analyses, our goal is to assess both the interpretability
and predictive capability of different statistical learning methods.

### 1.2 About Dataset

The dataset used in this study is the widely referenced *Medical
Insurance* dataset from Kaggle, consisting of **2,772 observations and 7
variables**. Each row represents one insured individual, and the
variables describe demographic information, lifestyle factors, and the
corresponding medical insurance charges. The dataset is commonly used as
a benchmark for regression modeling tasks due to its simplicity and
interpretability.

The variables included in the dataset are:

| Variable | Description |
|---------------------------------|---------------------------------------|
| **age** | Age of the individual |
| **sex** | Gender (male/female) |
| **bmi** | Body Mass Index, an indicator of body fat |
| **children** | Number of dependent children covered by the insurance |
| **smoker** | Smoking status (smoker / non-smoker) |
| **region** | Residential region in the United States (northeast, northwest, southeast, southwest) |
| **charges** | Total medical insurance cost (target variable) |

This dataset allows us to examine how demographic and lifestyle
characteristics influence medical expenses. In particular, variables
such as **BMI** and **smoking status** are known to play significant
roles in driving healthcare costs. The dataset is suitable for
evaluating both interpretable statistical models and more flexible
machine learning methods.

## 2. Exploratory Data Analysis (EDA)

-   Libraries

```{r,warning=FALSE,message=FALSE}
# ============================
# Load libraries
# ============================

#install.packages("caret")
library(glmnet)
library(ggplot2)
library(dplyr)
library(tidyr)
library(ggforce)
library(gridExtra)
library(reshape2)
library(GGally)
library(corrplot)
library(e1071)
library(rpart)
library(rpart.plot)
library(tree)
library(randomForest)
library(caret)
library(xgboost)
library(Matrix)
library(caTools)
library(leaps)
library(scales)
```

### 2.1. Load and Inspect the dataset

```{r}
# ============================
# Load and inspect the dataset
# ============================

# Load dataset
Insurance <- read.csv("medical_insurance.csv", stringsAsFactors = FALSE)

# Quick structure and preview
str(Insurance)          # Check variable types
head(Insurance)         # Preview first few rows
summary(Insurance)      # Summary statistics

# ============================
# Check and handle missing values
# ============================

# Count missing values in each column
na_counts <- sum(is.na(Insurance))
na_counts # 0 NA data

# Count how many rows have at least one missing value
missing_rows <- sum(!complete.cases(Insurance))
missing_rows # 0 missing data

```

### 2.2. Univariate Analysis

```{r}
# ============================
# Univariate Analysis
# ============================

Insurance_view <- Insurance

# convert 0/1 columns to factors
#discrete_cols <- c("hypertension", "diabetes", "asthma", "copd", "cardiovascular_disease", "cancer_history", "kidney_disease", "liver_disease", "arthritis", "mental_health", "is_high_risk", "had_major_procedure")
discrete_cols <- c()
#Insurance_view[discrete_cols] <- lapply(Insurance_view[discrete_cols], factor)


# Convert all character columns to factors
for (v in names(Insurance_view)) {
  if (is.character(Insurance_view[[v]])) {
    Insurance_view[[v]] <- factor(Insurance_view[[v]])
  }
}


num_cols <- names(Insurance_view)[sapply(Insurance_view, is.numeric)]
cat_cols <- names(Insurance_view)[sapply(Insurance_view, is.factor)]

plot_list <- list()

# ---- Numeric Variable: Histograms ----
for (v in num_cols) {
  p <- ggplot(Insurance_view, aes(x = .data[[v]])) + 
    geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "steelblue", alpha = 0.7) +
    theme(axis.text.x = element_text(size = 4)) +

    theme_minimal()

  plot_list[[v]] <- p
}

# ---- Category Variable: Barplots ----
for (v in cat_cols) {
  p <- ggplot(Insurance_view, aes(x = .data[[v]])) + 
    geom_bar(fill = "lightgreen", color = "black") +
    geom_text(stat='count', aes(label = after_stat(count)), vjust = -0.5, size = 2) +
    theme(axis.text.x = element_text(size = 6,angle = 45, hjust = 1)) +
    theme_minimal()
  
  plot_list[[v]] <- p
}

plot_row <- 2
plot_col <- 2
plot_total <- plot_col*plot_row
num_plot <- length(plot_list)
num_pages <- floor(num_plot / plot_total)

for (page in 1:num_pages) {
  beg <- (page-1) * plot_total + 1; end <- page * plot_total
  grid.arrange(grobs = plot_list[beg:end], 
               ncol = plot_col, nrow = plot_row)
  if (page == num_pages && num_plot %% plot_total > 0) {
    grid.arrange(grobs = plot_list[(end+1):num_plot], ncol = plot_col)
  }
}




```
The histograms show that age is roughly uniformly distributed, while BMI follows a slightly right-skewed bell shape. The number of children is heavily concentrated in the 0–2 range. Medical charges exhibit a strong right skew with many low-cost individuals and a long tail of high-cost cases, suggesting that log-transformation may be helpful. The barplots indicate that sex and region are well balanced, whereas smoking status is highly imbalanced, with far fewer smokers—yet smokers typically incur substantially higher charges, making this variable highly predictive.

```{r}
# Assuming your dataset is named 'Insurance'
Insurance_view <- Insurance

# ============================
# 0. Data Preprocessing (Preserving your logic)
# ============================
# Convert all character columns to factors
for (v in names(Insurance_view)) {
  if (is.character(Insurance_view[[v]])) {
    Insurance_view[[v]] <- factor(Insurance_view[[v]])
  }
}

# ============================
# 1. Q-Q Plots (For continuous variables)
# ============================

# 1.1 BMI Q-Q Plot
p_qq_bmi <- ggplot(Insurance_view, aes(sample = bmi)) +
  stat_qq(color = "steelblue", alpha = 0.5) +
  stat_qq_line(color = "red", lwd = 1) +
  labs(title = "Q-Q Plot: BMI", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_minimal()

# 1.2 Log(Charges) Q-Q Plot
# Note: Using natural log log() here; replace with log10 if needed
p_qq_charges <- ggplot(Insurance_view, aes(sample = log1p(charges))) +
  stat_qq(color = "steelblue", alpha = 0.5) +
  stat_qq_line(color = "red", lwd = 1) +
  labs(title = "Q-Q Plot: Log1p(Charges)", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_minimal()

# Display Q-Q Plots
grid.arrange(p_qq_bmi, p_qq_charges, ncol = 2)
```
The Q-Q plot for BMI shows mild deviation from normality mainly in the upper tail, indicating a slightly right-skewed distribution. After applying log1p transformation to medical charges, the Q-Q plot aligns much more closely with the theoretical normal line, although minor deviations remain at the extremes. This confirms that log transformation effectively reduces heavy skewness and stabilizes variance in charges.

```{r}
# ============================
# 2. Pie Charts (For categorical variables)
# ============================

# Define a function to draw pie charts to avoid repeating code
draw_pie_chart <- function(data, col_name) {
  # 1. Calculate counts and percentages
  plot_data <- data %>%
    count(.data[[col_name]]) %>%
    mutate(
      perc = n / sum(n),
      labels = scales::percent(perc) # Convert to percentage format (e.g., 50%)
    ) %>%
    arrange(desc(perc)) # Sort to make the chart look better

  # 2. Plotting
  ggplot(plot_data, aes(x = "", y = perc, fill = .data[[col_name]])) +
    geom_bar(width = 1, stat = "identity", color = "white") +
    coord_polar("y", start = 0) +
    # Add text labels (positioned in the middle of the slices)
    geom_text(aes(label = labels),
              position = position_stack(vjust = 0.5),
              size = 4, color = "black") +
    labs(title = paste("Pie Chart:", col_name), x = NULL, y = NULL, fill = col_name) +
    theme_void() + # Pie charts usually don't need axis backgrounds; void theme is cleanest
    theme(plot.title = element_text(hjust = 0.5, face = "bold")) +
    scale_fill_brewer(palette = "Pastel1") # Use a pastel color palette
}

# Draw specific pie charts
pie_sex <- draw_pie_chart(Insurance_view, "sex")
pie_smoker <- draw_pie_chart(Insurance_view, "smoker")
pie_region <- draw_pie_chart(Insurance_view, "region")

# Display pie charts (Layout: two on top, one on bottom)
grid.arrange(pie_sex, pie_smoker, pie_region,
             layout_matrix = rbind(c(1, 2), c(3, 3)))
```
The pie charts show that sex is almost evenly split between male and female, indicating a balanced sample. Smoking status is highly imbalanced, with only about 20% smokers, but this minority group is known to have much higher medical costs. The four regions are distributed fairly evenly, showing no major geographic sampling bias.

### 2.3. Bivariate Analysis

```{r}
# ============================
# Bivariate Analysis
# ============================

# ---- Feature Correlation ----


# compute correlation coefficients of "annual_medical_cost" to other features
cor_results <- sapply(c(num_cols, discrete_cols), function(v) {
  cor(Insurance[[v]], 
      Insurance$charges, use = "complete.obs")
})

cor_table <- data.frame(
  feature = names(cor_results),
  corr = as.numeric(cor_results)
)

# Sorting by "abs(corr)"
cor_table <- cor_table[order(abs(cor_table$corr), decreasing = TRUE), ]

# correlation coefficients table
cor_table
```
The correlation table shows that age has the strongest positive correlation with medical charges, followed by BMI, while the number of children has only a weak relationship with charges. These results indicate that age and BMI are more informative predictors, whereas children contributes relatively little linear explanatory power.

```{r}
# ---- Correlation Heatmap ----
#top20_feature <- cor_table[1:20, ]$feature


corr_mat <- cor(Insurance[cor_table$feature], use = "complete.obs")

corrplot(corr_mat, method = "color",
         tl.col = "black",
         
         addCoef.col = "black")

```
The heatmap confirms that medical charges have the strongest linear relationship with age and BMI, while children shows only a very weak correlation. The predictors themselves exhibit low inter-correlation, suggesting minimal multicollinearity and indicating that each variable provides mostly independent information.

```{r}
# ============================
# 1. Categorical Variables vs Charges
# Use Boxplot to visualize distribution and outliers
# ============================

# Define a plotting function to avoid code duplication
plot_box <- function(data, x_col, y_col, fill_color) {
  # Use as.factor() to force the x-axis variable into a categorical factor
  # This allows scale_fill_brewer to work and ensures the Boxplot groups correctly
  ggplot(data, aes(x = as.factor(.data[[x_col]]), 
                   y = .data[[y_col]], 
                   fill = as.factor(.data[[x_col]]))) +
    
    geom_boxplot(alpha = 0.7, outlier.shape = NA) + 
    stat_summary(fun = mean, geom = "point", shape = 23, size = 3, fill = "white") + 
    
    # Manually set the x-label back to the original column name; otherwise, it would display "as.factor(children)"
    labs(title = paste(x_col, "vs", y_col), 
         x = x_col, 
         y = y_col, 
         fill = x_col) + # Set legend title
    
    theme_minimal() +
    theme(legend.position = "none") + # Boxplots usually don't need a legend since the x-axis is already labeled
    scale_fill_brewer(palette = fill_color)
}

# Re-run plotting
p1 <- plot_box(Insurance_view, "smoker", "charges", "Set1")
p2 <- plot_box(Insurance_view, "sex", "charges", "Pastel1")
p3 <- plot_box(Insurance_view, "region", "charges", "Set2")
# This line should work correctly now because 'children' is converted to a factor
p4 <- plot_box(Insurance_view, "children", "charges", "Spectral") 

grid.arrange(p1, p2, p3, p4, ncol = 2)
```
The boxplots show that smokers have substantially higher medical charges than non-smokers, making smoking status the strongest categorical predictor. Sex has only a small difference in median charges, and regional differences are minimal. The number of children also shows no clear trend, indicating limited influence on charges compared to other factors.

### 2.4 Analysis of Interaction Effects

```{r}
Insurance_view$log_charges <- log1p(Insurance_view$charges)
# ============================
# 1. BMI * Smoker 
# ============================

p_bmi_smoker <- ggplot(Insurance_view, aes(x = bmi, y = charges, color = smoker)) +
  geom_point(alpha = 0.5, size = 1) +
  geom_smooth(method = "lm", se = FALSE, size = 1.2) +
  scale_color_manual(values = c("yes" = "#c0392b", "no" = "#2980b9")) +
  labs(title = "1. Interaction: BMI * Smoker",
       x = "BMI", y = "Charges") +
  theme_minimal() +
  theme(legend.position = "top")


p_bmi_smoker
```
The interaction plot shows that among smokers, higher BMI is strongly associated with higher medical charges, reflected by the steep positive slope. In contrast, non-smokers show almost no relationship between BMI and charges. This indicates a strong interaction effect: BMI matters much more for smokers, making the combined effect of BMI and smoking highly predictive.

```{r}
# ============================
# 1. BMI * Smoker 
# ============================
log_p_bmi_smoker <- ggplot(Insurance_view, aes(x = bmi, y = log_charges, color = smoker)) +
  geom_point(alpha = 0.5, size = 1) +
  geom_smooth(method = "lm", se = FALSE, size = 1.2) +
  scale_color_manual(values = c("yes" = "#c0392b", "no" = "#2980b9")) +
  labs(title = "1. Interaction: BMI * Smoker",
       x = "BMI", y = "Log1p(Charges)") +
  theme_minimal() +
  theme(legend.position = "top")
log_p_bmi_smoker
```
After applying the log1p transformation, the interaction pattern remains clear: smokers show a strong positive relationship between BMI and medical costs, while non-smokers exhibit a nearly flat trend. The transformation reduces the influence of extreme values but preserves the interaction, confirming that BMI affects smokers much more than non-smokers.

```{r}
# ============================
# 2. Age * Smoker 
# ============================
p_age_smoker <- ggplot(Insurance_view, aes(x = age, y = charges, color = smoker)) +
  geom_point(alpha = 0.5, size = 1) +
  geom_smooth(method = "lm", se = FALSE, size = 1.2) +
  scale_color_manual(values = c("yes" = "#c0392b", "no" = "#2980b9")) +
  labs(title = "2. Interaction: Age * Smoker",
       x = "Age", y = "Charges") +
  theme_minimal() +
  theme(legend.position = "top")

p_age_smoker
```
The interaction plot shows that charges increase with age for both smokers and non-smokers, but smokers consistently incur much higher costs at every age level. Although the slopes are similar, the vertical gap between the two groups indicates a strong additive effect of smoking, meaning smoking elevates medical charges regardless of age.

```{r}
# ============================
# 2. Age * Smoker 
# ============================
log_p_age_smoker <- ggplot(Insurance_view, aes(x = age, y = log_charges, color = smoker)) +
  geom_point(alpha = 0.5, size = 1) +
  geom_smooth(method = "lm", se = FALSE, size = 1.2) +
  scale_color_manual(values = c("yes" = "#c0392b", "no" = "#2980b9")) +
  labs(title = "2. Interaction: Age * Smoker",
       x = "Age", y = "Log1p(Charges)") +
  theme_minimal() +
  theme(legend.position = "top")

log_p_age_smoker
```
After log transformation, the interaction pattern becomes clearer: medical costs increase steadily with age for both groups, but smokers consistently remain at a higher cost level across all ages. The nearly parallel lines show that aging raises costs similarly for smokers and non-smokers, while smoking adds a strong upward shift regardless of age.

```{r}
# ============================
# Special Handling: To plot bmi:age, we need to group age
# ============================
# Divide age into three stages: Young (18-35), Middle (36-55), Senior (55+)
Insurance_view <- Insurance_view %>%
  mutate(age_group = cut(age, 
                         breaks = c(0, 35, 55, 100), 
                         labels = c("Young (18-35)", "Middle (36-55)", "Senior (55+)")))
# ============================
# 3. BMI * Age 
# ============================
# Here we use the age_group we just created for grouping
p_bmi_age <- ggplot(Insurance_view, aes(x = bmi, y = charges, color = age_group)) +
  geom_point(alpha = 0.3, size = 1) +
  geom_smooth(method = "lm", se = FALSE, size = 1.2) +
  scale_color_brewer(palette = "Dark2") + # Use high-contrast colors
  labs(title = "3. Interaction: BMI * Age (Grouped)",
       x = "BMI", y = "Charges", color = "Age Group") +
  theme_minimal() +
  theme(legend.position = "top")

p_bmi_age
```
The interaction plot shows that medical charges increase with BMI across all age groups, but older individuals generally incur higher costs at any BMI level. The senior group has the highest cost baseline, followed by the middle-aged group, while the young group remains lowest. Although the slopes are similar, the vertical separation indicates that age amplifies overall medical costs independently of BMI.

```{r}
# ============================
# 3. BMI * Age 
# ============================
# Here we use the age_group we just created for grouping
p_bmi_age <- ggplot(Insurance_view, aes(x = bmi, y = log_charges, color = age_group)) +
  geom_point(alpha = 0.3, size = 1) +
  geom_smooth(method = "lm", se = FALSE, size = 1.2) +
  scale_color_brewer(palette = "Dark2") + # 使用高對比顏色
  labs(title = "3. Interaction: BMI * Age (Grouped)",
       x = "BMI", y = "Log1p(Charges)", color = "Age Group") +
  theme_minimal() +
  theme(legend.position = "top")

p_bmi_age
```
After log transformation, the three age groups remain clearly separated, with seniors consistently showing the highest cost levels, followed by the middle group and then the young group. All groups show a mild upward trend with BMI, but the vertical gaps dominate the pattern, indicating that age contributes more strongly to cost differences than BMI does. The log transformation reduces extreme variation but preserves the overall interaction structure.

## 3. Method

-   Data Preprocessing

```{r}

# ============================
# Data Preprocessing
# ============================


# data frame with all avaliable feature
df_raw <- Insurance

# Convert all character columns to factors
for (v in names(df_raw)) {
  if (is.character(df_raw[[v]])) {
    df_raw[[v]] <- factor(df_raw[[v]])
  }
}


# convert 0/1 columns to factors
discrete_cols <- c()

non_char_cols <- names(df_raw)[sapply(df_raw, is.numeric)]
num_cols <- setdiff(non_char_cols, discrete_cols)

# ---- One-hot encoding (dummy variables) ----
 # This turns each factor into 0/1 columns
df_ori <- model.matrix(~ ., data = df_raw)[,-1]

# Final cleaned numeric dataset
df_ori <- as.data.frame(df_ori)
# convert col names to valid names
names(df_ori) <- make.names(names(df_ori))



# View origininal dataset
str(df_ori)
head(df_ori)

# ---- Data Transformation ----
df_trans <- df_ori

for(v in num_cols) {
  if(skewness(df_trans[,v]) > 1) {
    # log1p transform
    df_trans[,v] <- log1p(df_trans[,v])
    
    before_trans <- ggplot(df_ori, aes(x = .data[[v]])) + 
    geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "steelblue", alpha = 0.7) +
    theme(axis.text.x = element_text(size = 4)) +
    theme_minimal() + 
    labs(
      title = paste("Before Transformation:", v)
    )
    after_trans <- ggplot(df_trans, aes(x = .data[[v]])) + 
    geom_histogram(aes(y = after_stat(density)), bins = 30, 
                   fill = "steelblue", alpha = 0.7) +
    theme(axis.text.x = element_text(size = 4)) +
    theme_minimal() +
    labs(
      title = paste("After Transformation:", v),
      x = paste0("log1p(", v, ")")      # x label 直接寫清楚
    )
    grid.arrange(grobs = list(before_trans, after_trans), ncol = 2)

  }
}



```

We aim to categorical data perform dummy one-hot encoding. Then, for the
right-skew numerical data, we transform data by log1p().

-   Data Splitting

```{r split-data}

set.seed(123)

# Number of observations
n <- nrow(df_trans)

# 80% for training, 20% for test
train_idx <- sample(seq_len(n), size = 0.8 * n)

train <- df_trans[train_idx, ]
test  <- df_trans[-train_idx, ]

train_ori <- df_ori[train_idx, ]
test_ori  <- df_ori[-train_idx, ]
```

We randomly split the cleaned dataset into a 80% training set and a 20%
test set using a fixed random seed to ensure reproducibility. All models
in this section are trained on the training set and evaluated on the
test set for fair comparison.

-   Some Useful Function

```{r}
# RMSE
computeRMSE <- function(y_pred, y_true, transform = FALSE) {
  if (transform == TRUE) {
    y_pred <- expm1(y_pred)
    y_true <- expm1(y_true)
  }
  rmse <- sqrt(mean((y_pred - y_true)^2))
    
  return(rmse)
}

# R^2
computeR2 <- function(y_pred, y_true, transform = FALSE) {
  if (transform == TRUE) {
    y_pred <- expm1(y_pred)
    y_true <- expm1(y_true)
  }
  
  SSE <- sum((y_true - y_pred)^2)
  SST <- sum((y_true - mean(y_true))^2)
  
  R2 <- 1 - SSE/SST
  
  return(R2)
}

```

### 3.1 Linear Regression Model (Parametric)

#### 3.1.1 Multiple Linear Regression Model(Baseline)

```{r baseline-ols}
# Response variable
#response_var <- "annual_medical_cost"
response_var <- "charges"
# Variables that would leak cost information — must be excluded
# sus_vars <- c("annual_premium", "monthly_premium", "claims_count", "avg_claim_amount", "total_claims_paid")
leakage_vars <- c()


# Construct predictor set
predictor_vars <- setdiff(
  names(df_trans),
  c(response_var, leakage_vars)
)



# Build model formula
form_baseline <- as.formula(
  paste(response_var, "~", paste(predictor_vars, collapse = "+"))
)

# Fit OLS
lm_full <- lm(form_baseline, data = train)
summary(lm_full)

# Predictions
train_pred_lm <- predict(lm_full, newdata = train)
test_pred_lm  <- predict(lm_full, newdata = test)

# Compute R^2
y_train <- train[[response_var]]
y_test  <- test[[response_var]]

R2_train_lm <- computeR2(train_pred_lm, y_train, TRUE)
#  1 - sum((y_train - train_pred_lm)^2) / sum((y_train - mean(y_train))^2)

R2_test_lm  <- computeR2(test_pred_lm, y_test, TRUE)
# 1 - sum((y_test - test_pred_lm)^2) / sum((y_test - mean(y_test))^2)

# Compute RMSE
RMSE_train_lm <- computeRMSE(train_pred_lm, y_train, TRUE)
RMSE_test_lm  <- computeRMSE(test_pred_lm, y_test, TRUE)

R2_train_lm; R2_test_lm
RMSE_train_lm; RMSE_test_lm
```

The baseline OLS model utilizes all transformed predictors to estimate
medical charges. The model achieves a training R\^2 = 0.53 and test R\^2
= 0.51, indicating that the linear structure explains only about half of
the variance in medical costs. The RMSE values=8284.48 on the training
set and 8644.52 on the test set show that substantial prediction error
remains.

Significant predictors include age, BMI, number of children, smoking
status, and certain regional indicators, all aligning with expected
relationships in healthcare expenditure. However, the large and uneven
residuals suggest the presence of nonlinear patterns and
heteroscedasticity that the linear model fails to capture. These
findings imply that more flexible modeling approaches may be required to
achieve improved predictive performance.

#### 3.1.2 Lasso Linear Regression Model with CV

```{r lasso-cv-setup}

# Construct design matrices (remove intercept)
x_train_base <- model.matrix(form_baseline, data = train)[, -1]
x_test_base  <- model.matrix(form_baseline, data = test)[, -1]
head(x_train_base)
head(x_test_base)
```

```{r lasso-cv-fit}
set.seed(123)

cv_lasso_base <- cv.glmnet(
  x = x_train_base,
  y = y_train,
  alpha = 1,        # Lasso
  nfolds = 10,
  standardize = TRUE
)

lambda_min_base <- cv_lasso_base$lambda.min

cat("Best lambda: ", lambda_min_base, '\n')

coef(cv_lasso_base, s = lambda_min_base)

```

In the Lasso model with 10-fold cross-validation, the selected penalty
parameter is very small:

$\lambda_{\min} \approx$ 0.0013

Such a small λ implies very weak regularization, so the Lasso solution
is expected to be very close to the OLS solution. In other words,
cross-validation suggests that the data do not benefit much from strong
shrinkage on the coefficients under this feature scaling. As a result,
the fitted coefficients and overall performance of the Lasso model are
almost the same as those of the baseline linear regression.

```{r lasso-cv-performance}
# Predictions for CV-selected lambda.min
train_pred_cv_base <- predict(cv_lasso_base, newx = x_train_base, s = "lambda.min")
test_pred_cv_base  <- predict(cv_lasso_base, newx = x_test_base,  s = "lambda.min")

# Compute R^2
R2_train_cv_base <- computeR2(y_train, train_pred_cv_base, TRUE)
#  1 - sum((y_train - train_pred_cv_base)^2) / sum((y_train - mean(y_train))^2)

R2_test_cv_base  <- computeR2(y_test, test_pred_cv_base, TRUE)
#  1 - sum((y_test - test_pred_cv_base)^2) / sum((y_test - mean(y_test))^2)

# Compute RMSE
RMSE_train_cv_base <- computeRMSE(y_train, train_pred_cv_base, TRUE)
RMSE_test_cv_base  <- computeRMSE(y_test, test_pred_cv_base, TRUE)

R2_train_cv_base; R2_test_cv_base
RMSE_train_cv_base; RMSE_test_cv_base
```

The model achieves $R^2_{\text{train}}$ = 0.636 and $R^2_{\text{test}}$
= 0.568, which are similar to the baseline OLS model. The RMSE values
(training: 8244.83, test: 8591.93) show no meaningful reduction in
prediction error.

Because the dataset contains a modest number of predictors and exhibits
notable nonlinear patterns, Lasso does not address the underlying model
misspecification. Residual diagnostics continue to display curved
structures, indicating that the linear assumption is insufficient.

#### 3.1.3 Lasso Regression Model with BIC

```{r fit full Lasso path & compute BIC}
# Fit full Lasso path on baseline features
lasso_base <- glmnet(
  x = x_train_base,
  y = y_train,
  alpha = 1,
  standardize = TRUE
)


# Function to compute BIC for each lambda
calc_bic <- function(fit, x, y) {
  n <- length(y)
  y_hat <- predict(fit, newx = x)
  rss <- colSums((y - y_hat)^2)
  df  <- fit$df  # number of non-zero coefficients
  bic <- n * log(rss / n) + df * log(n)
  
  return(bic)
}

bic_base <- calc_bic(lasso_base, x_train_base, y_train)

lambda_bic_base <- lasso_base$lambda[which.min(bic_base)]

cat("Best lambda: ", lambda_bic_base, '\n')

coef(lasso_base, s = lambda_bic_base)


plot(log(lasso_base$lambda), bic_base, type = "l",
     xlab = "log(lambda)", ylab = "BIC")
abline(v = log(lambda_bic_base), lty = 2)
```

To select the optimal regularization strength, we evaluated the full
Lasso solution path using the Bayesian Information Criterion (BIC). The
λ that minimizes BIC is:

$\lambda_{\text{BIC}}$ = 0.001315

This value is extremely small, indicating that BIC favors very weak
regularization. Consequently, the selected model retains almost all
coefficients and behaves nearly identically to the CV-selected Lasso
model and even to the baseline OLS model.

```{r use BIC λ to evaluate performance}
# Predictions using BIC-selected lambda
train_pred_bic_base <- predict(lasso_base, newx = x_train_base, s = lambda_bic_base)
test_pred_bic_base  <- predict(lasso_base, newx = x_test_base,  s = lambda_bic_base)

# Compute R^2
R2_train_bic_base <- computeR2(y_train, train_pred_bic_base, TRUE)
# 1 - sum((y_train - train_pred_bic_base)^2) / sum((y_train - mean(y_train))^2)

R2_test_bic_base  <- computeR2(y_test, test_pred_bic_base, TRUE)
# 1 - sum((y_test - test_pred_bic_base)^2) / sum((y_test - mean(y_test))^2)

# Compute RMSE
RMSE_train_bic_base <- computeRMSE(y_train, train_pred_bic_base, TRUE)
RMSE_test_bic_base  <- computeRMSE(y_test, test_pred_bic_base, TRUE)

R2_train_bic_base; R2_test_bic_base
RMSE_train_bic_base; RMSE_test_bic_base
```

```         
•   Training R² = 0.636

•   Test R² = 0.568

•   Training RMSE = 8244.83

•   Test RMSE = 8591.93
```

Compared with the CV-selected Lasso model, the BIC-selected λ imposes
stronger regularization, resulting in a more sparse model with higher
bias. This leads to lower R² and higher RMSE on both training and test
sets.

```{r baseline-summary}


results_baseline <- data.frame(
  Model = c("OLS baseline",
            "Lasso (CV λ_min)",
            "Lasso (BIC)"),
  R2_test = c(R2_test_lm, R2_test_cv_base, R2_test_bic_base),
  RMSE_test = c(RMSE_test_lm, RMSE_test_cv_base, RMSE_test_bic_base)
)

results_baseline
```

Among the three baseline linear models, the Lasso versions perform
slightly better than OLS. The OLS model reaches an R² of about 0.51,
while Lasso improves it to 0.568 and reduces RMSE from 8644 to 8592,
meaning more accurate predictions. Interestingly, both CV and BIC choose
the same λ, so their results are identical. Overall, Lasso helps, but
the improvement is modest, suggesting that a simple linear model might
not fully capture the structure of the data and more flexible methods
may be needed later.

```{r}

# --- OLS (baseline) ---
results_ols <- data.frame(preds = expm1(test_pred_lm), actual = expm1(y_test))
results_ols$residuals <- results_ols$actual - results_ols$preds

# Prediction vs Actual
PvA_ols <- ggplot(results_ols, aes(x = preds, y = actual)) +
  geom_point(alpha = 0.5) +
  geom_abline(color = "red") +
  theme_minimal() +
  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+

  labs(title = "OLS Prediction vs Actual")

# Prediction vs Residuals
PvR_ols <- ggplot(results_ols, aes(x = preds, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red") +
  theme_minimal() +
  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  labs(title = "OLS Prediction vs Residuals")

grid.arrange(grobs = list(PvA_ols, PvR_ols), ncol = 2)
```

1.  

From the OLS prediction plot, the model captures the general upward
trend, but once the medical charges become large (around 30,000 and
above), the predictions start to fall noticeably below the 45-degree
reference line. This indicates that OLS systematically underestimates
high-cost patients.

The residual plot shows the issue even more clearly. Instead of being
randomly scattered around zero, the residuals form a curved, structured
pattern. Lower-cost observations tend to have positive residuals, while
higher-cost ones have increasingly negative residuals. This pattern
suggests that the model is missing important nonlinear relationships,
and its linear structure is not flexible enough to explain the full
behavior of medical costs.

OLS captures the overall trend but fails to model the nonlinear increase
in costs, leading to systematic underestimation for high-charge patients
and a characteristic curved residual pattern.

```{r}
# --- CV ---

test_pred_cv_base <- as.numeric(test_pred_cv_base)

results_cv <- data.frame(preds = expm1(test_pred_cv_base), actual = expm1(y_test))
results_cv$residuals <- results_cv$actual - results_cv$preds

# Prediction vs Actual
PvA_cv <- ggplot(results_cv, aes(x = preds, y = actual)) +
  geom_point(alpha = 0.5) +
  geom_abline(color = "red") +
  theme_minimal() +
  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+

  labs(title = "Best CV Prediction vs Actual")

# Prediction vs Residuals
PvR_cv <- ggplot(results_cv, aes(x = preds, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red") +
  theme_minimal() +
  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  labs(title = "Best CV Prediction vs Residuals")

grid.arrange(grobs = list(PvA_cv, PvR_cv), ncol = 2)
```

2.  

In the CV-selected Lasso model, the prediction plot still follows the
overall trend of the actual charges, but the pattern is very similar to
OLS. Once the true medical cost becomes large, the predictions remain
noticeably below the 45-degree reference line. This means that even
after applying Lasso regularization, the model still underestimates
high-cost patients.

The residual plot confirms this. The residuals do not scatter randomly
around zero; instead, they show a clear downward pattern as predictions
increase. This indicates that errors are systematically related to the
scale of the predicted cost. In other words, the model is still missing
key nonlinear relationships, and Lasso shrinkage alone is not enough to
correct the structural bias.

The Lasso model chosen by cross-validation performs slightly better
numerically, but the core issue remains—systematic underestimation for
high-spending individuals and a strong nonlinear residual pattern,
suggesting that a purely linear model is not flexible enough.

```{r}
# --- BIC ---
test_pred_bic_base <- as.numeric(test_pred_bic_base)

results_bic <- data.frame(preds = expm1(test_pred_bic_base), actual = expm1(y_test))
results_bic$residuals <- results_bic$actual - results_bic$preds

# Prediction vs Actual
PvA_bic <- ggplot(results_bic, aes(x = preds, y = actual)) +
  geom_point(alpha = 0.5) +
  geom_abline(color = "red") +
  theme_minimal() +
  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  labs(title = "Best BIC Prediction vs Actual")

# Prediction vs Residuals
PvR_bic <- ggplot(results_bic, aes(x = preds, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red") +
  theme_minimal() +
  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  labs(title = "Best BIC Prediction vs Residuals")
grid.arrange(grobs = list(PvA_bic, PvR_bic), ncol = 2)

```

3.  

The BIC-selected Lasso model shows prediction patterns almost identical
to the CV-selected Lasso, which makes sense because both methods choose
similar λ values. The model captures the general increasing trend, but
like the other models, it consistently under-predicts high-cost cases,
and the residual plot again shows a clear downward pattern. This
indicates remaining nonlinearity in the data that linear
models—including penalized ones—cannot fully handle.

Summary

Across the baseline OLS model and the Lasso models selected by CV and
BIC, the overall predictive performance remains limited. Although Lasso
slightly improves R² and reduces RMSE compared to OLS, all three models
show the same systematic issue: they consistently underestimate high
medical charges, and their residuals reveal a strong nonlinear pattern
that linear models cannot capture. This suggests that medical cost data
contains complex relationships—likely interactions or nonlinear
effects—that are beyond the capability of linear or penalized linear
models.


#### 3.1.4 Feature Engineering and Model Selection

##### Regression with interaction terms

Based on our EDA findings (Section 2.4), we identified a significant
interaction between BMI and smoking status. The previous linear models
(3.1.1 - 3.1.3) failed to capture this non-linear relationship,
resulting in underestimation of high medical costs. In this section, we
perform feature engineering by explicitly adding interaction terms and
utilize model selection algorithms to identify the optimal subset of
predictors.

```{r}
train_set = train[,]
test_set = test[,]
# add interaction term to original dataset
train_set$bmi_smoker <- train_set$bmi * train_set$smokeryes
test_set$bmi_smoker  <- test_set$bmi * test_set$smokeryes

train_set$age_smoker <- train_set$age * train_set$smokeryes
test_set$age_smoker  <- test_set$age * test_set$smokeryes

str(train_set)
```
In the below plot of different model performance metrics to the number of variables. When feature subset size = 8, the regression model has the lowest test RMSE. The selected features are the 2 interaction terms and other 6 features, removing `bmi` and `regionnorthwest`.
```{r}
set.seed(123)
regfit.full = regsubsets(charges ~ ., data = train_set, nvmax = 12)
reg1.summary = summary(regfit.full)
reg1.summary
num_models <- length(reg1.summary$rss)
```
```{r}
par(mfrow=c(1,2))
index = which.max(reg1.summary$adjr2)
cat("\nSize:", index, "\n")
cat("Adjusted RSq:", reg1.summary$adjr2[index],"\n")
coef(regfit.full,index)
plot(reg1.summary$adjr2, xlab='Number of Variables',ylab='Adjusted RSq',type='b', pch=19)
points(index,reg1.summary$adjr2[index],col = "red", pch=19)


index = which.min(reg1.summary$bic)
cat("\nSize:", index, "\n")
cat("BIC:", reg1.summary$bic[index],"\n")
coef(regfit.full,index)
plot(reg1.summary$bic, xlab='Number of Variables',ylab='BIC',type='b', pch=19)
points(index,reg1.summary$bic[index],col = "red", pch=19)

```
```{r}
par(mfrow=c(1,2))

test.mat = model.matrix(charges~.,data = test_set)
test.errors = rep(NA,num_models)
for(i in 1:num_models){
  coefi = coef(regfit.full,id=i)
  intercept = as.vector(coefi)[1]
  pred = test.mat[,names(coefi)] %*% coefi
  test.errors[i] = sqrt(mean((test_set$charges - pred)^2))
}
min.index = which.min(reg1.summary$rss)
cat("\nSize:", min.index, "\n")
cat("Train RMSE:", sqrt(reg1.summary$rss[min.index]/n),"\n")
coef(regfit.full,id=min.index)
plot(sqrt((reg1.summary$rss)/n), xlab='Number of Variables',ylab='Train RMSE',type='b',pch=19)
points(min.index, sqrt(reg1.summary$rss[min.index]/n),col = "red", pch=19)

min.index = which.min(test.errors)
cat("\nSize:", min.index, "\n")
cat("Test RMSE:", test.errors[min.index],"\n")
coef(regfit.full,id=min.index)
plot(1:num_models, test.errors,xlab='Number of variables', ylab='Test RMSE',type="b", pch=19)
points(min.index, test.errors[min.index],col = "red", pch=19)
```
**Result**
-   Linear regression model with 8 predictors by feature selection:
```{r}
predictors_fs <- setdiff(names(train_set), c('charges', 'regionnorthwest','bmi'))
form_fs <- as.formula(
  paste('charges ~', paste(predictors_fs, collapse = "+")))
lm_fs <- lm(form_fs,data = train_set)
summary(lm_fs)

# Predictions
train_pred_fs <- predict(lm_fs, newdata = train_set)
test_pred_fs  <- predict(lm_fs, newdata = test_set)

# Compute R^2
y_train <- train$charges
y_test  <- test$charges

R2_train_fs2 <- computeR2(train_pred_fs, y_train, TRUE)
R2_test_fs2  <- computeR2(test_pred_fs, y_test, TRUE) 

# Compute RMSE
RMSE_train_fs2 <- computeRMSE(train_pred_fs, y_train, TRUE)
RMSE_test_fs2  <- computeRMSE(test_pred_fs, y_test, TRUE)

cat('Train R^2:',R2_train_fs2,'\n')
cat('Test R^2:',R2_test_fs2,'\n')
cat('Train RMSE:',RMSE_train_fs2,'\n')
cat('Test RMSE:',RMSE_test_fs2,'\n')
```
The linear regression model fits the data
well, explaining approximately 81.4% of the variance in the
log-transformed medical charges (Adjusted $R^2 = 0.8144$). The
F-statistic indicates the model is statistically significant
($p < 2.2e^{-16}$). There is no evidence of significant overfitting, as
the $R^2$ on the test set ($0.778$) is comparable to the training set
($0.794$).

Impact of Log-Transformation

Since the dependent variable is transformed
using log1p, the coefficients should be interpreted as approximate
percentage changes in medical charges (for small coefficients) or
requiring exponentiation ($e^\beta - 1$) for larger effects.

Key Drivers and Interactions

-   Smoking Status: This is the most dominant factor. The base
    coefficient for smokeryes (1.861) suggests that, holding other
    variables constant, smoking leads to a massive increase in predicted
    charges.

-   BMI and Smoking Interaction: The interaction term bmi:smoker is highly
    significant and positive ($0.053$). This indicates that BMI
    significantly increases medical costs only for smokers, acting as a
    major risk multiplier.

-   Age Dynamics: Generally, costs increase with age (age coeff:
    $0.041$). However, the negative interaction term age:smoker
    ($-0.033$) suggests that the rate of cost increase due to age is
    actually less steep for smokers compared to non-smokers, likely
    because smokers already start at a much higher cost baseline.

##### Regression with pairwise interaction
  In the last section, we manually add interaction terms as features, and use function for model selection, which can find the optimal feature subset. Now we try to put all pairwise interaction terms into consideration, and see if some feature interactions are influential in the linear regression model prediction. The regression model will take all predictors and all pairwise interaction terms as features, which is 36 features to be exact. We use the Lasso regression, as it can do automatic feature selection and prevent over-fitting. We also implement 10-fold cross validation to find the optimal lambda value in the Lasso model. This helps to learn a regression model that captures interaction terms' relationship fairly and generalizes well. 

The cv result shows that lambda.min is 0.000442, and it leads to 32 nonzero features. lambda.1se is 0.011482, and it leads to less nonzero features(=14), as the larger lambda values increase the L1 penalty and shrink more coefficients toward zero. Also, the cv MSE of lambda.min is smaller than lambda.1se.

```{r}
library(glmnet)
set.seed(123)

x_train <- model.matrix(charges ~ (.)^2, data = train)[, -1]
y_train <- train$charges
x_test <- model.matrix(charges ~ (.)^2, data = test)[,-1]
y_test <- test$charges

cv_lasso <- cv.glmnet(
  x = x_train,
  y = y_train,
  alpha = 1,        # Lasso
  nfolds = 10,
  standardize = TRUE
)

lambda_min <- cv_lasso$lambda.min
lambda_1se <- cv_lasso$lambda.1se
cv_lasso
#cat("Best lambda: ", lambda_min, '\n')
#coef(cv_lasso, s = lambda_min)           # this shows coefficients estimates
#cat("1se lambda: ", lambda_1se, '\n')
#coef(cv_lasso, s = lambda_1se)
```
```{r}
plot(cv_lasso)
```
**Result**
The Lasso regression model using lambda.min has smaller Train RMSE(5128.832<5541.604) and Test RMSE(5231.194<5730.521) than using lambda.1se. Also, lambda.min's model has larger Train/Test R^2 than that of lambda.1se's model. I think this is because smaller lambda values can result in a more complex regression model and it can learn more complex and crucial pattern of the data, and therefore reduce the predictive error. 

```{r lasso-cv-performance}
# Predictions for CV-selected lambda.1se: 	
# largest value of lambda such that error is within 1 standard error of the minimum.
train_pred_cv <- predict(cv_lasso, newx = x_train, s = "lambda.1se")
test_pred_cv  <- predict(cv_lasso, newx = x_test,  s = "lambda.1se")

# Compute R^2
R2_train_2way_lasso <- computeR2(y_train, train_pred_cv, TRUE)
R2_test_2way_lasso  <- computeR2(y_test, test_pred_cv, TRUE)

# Compute RMSE
RMSE_train_2way_lasso <- computeRMSE(y_train, train_pred_cv, TRUE)
RMSE_test_2way_lasso  <- computeRMSE(y_test, test_pred_cv, TRUE)
cat('Select lambda.1se (λ =',lambda_1se,')\n')
cat('Train R^2:',R2_train_2way_lasso,'\n')
cat('Test R^2:',R2_test_2way_lasso,'\n')
cat('Train RMSE:',RMSE_train_2way_lasso,'\n')
cat('Test RMSE:',RMSE_test_2way_lasso,'\n')
###########################################
# Predictions for CV-selected lambda.min
train_pred_cv <- predict(cv_lasso, newx = x_train, s = "lambda.min")
test_pred_cv  <- predict(cv_lasso, newx = x_test,  s = "lambda.min")

# Compute R^2
R2_train_2way_lasso <- computeR2(y_train, train_pred_cv, TRUE)
R2_test_2way_lasso  <- computeR2(y_test, test_pred_cv, TRUE)

# Compute RMSE
RMSE_train_2way_lasso <- computeRMSE(y_train, train_pred_cv, TRUE)
RMSE_test_2way_lasso  <- computeRMSE(y_test, test_pred_cv, TRUE)
cat('\nSelect lambda.min (λ =',lambda_min,')\n')
cat('Train R^2:',R2_train_2way_lasso,'\n')
cat('Test R^2:',R2_test_2way_lasso,'\n')
cat('Train RMSE:',RMSE_train_2way_lasso,'\n')
cat('Test RMSE:',RMSE_test_2way_lasso,'\n')
```

```{r}
coef(cv_lasso, s = lambda_min)
```
```{r}
feat =  c('age',                            
'sexmale' ,                       
'bmi'      ,                      
'children'  ,                     
'smokeryes'  ,                    
'regionnorthwest',                
'regionsouthwest'  ,              
'age:sexmale'       ,             
'age:bmi'            ,            
'age:children'        ,           
'age:smokeryes'        ,          
'age:regionnorthwest'   ,         
'age:regionsoutheast'    ,        
'age:regionsouthwest'     ,       
'sexmale:bmi'              ,      
'sexmale:children'          ,     
'sexmale:smokeryes'          ,    
'sexmale:regionnorthwest'     ,   
'sexmale:regionsoutheast'      ,  
'sexmale:regionsouthwest'       , 
'bmi:children'                   ,
'bmi:smokeryes'                  ,
'bmi:regionnorthwest'            ,
'bmi:regionsoutheast'            ,
'bmi:regionsouthwest'            ,
'children:smokeryes'             ,
'children:regionnorthwest'       ,
'children:regionsoutheast'       ,
'children:regionsouthwest'       ,
'smokeryes:regionnorthwest'      ,
'smokeryes:regionsoutheast'      ,
'smokeryes:regionsouthwest'      )

form_fs_lasso <- as.formula(
  paste(response_var, "~", paste(feat, collapse = "+"))
)
lm_fs_lasso <- lm(form_fs_lasso,data = train_set)
summary(lm_fs_lasso)
```

### 3.2 Tree-based Model (Non-parametric)

Tree-based models are more flexible non-parametric methods that don't
assume a specific functional form between predictors and the response
variable. Because they don't rely on data distributional assumptions,
these methods are generally more robust in the presence of outliers.
Therefore, we apply tree-based models to effectively capture the
non-linear and complex relationships in our dataset. In this section, we
apply two tree-based models. In particular, we will
train random forest and XGBoost models, representing aggregation and
boosting methods, respectively. Later, we will also compare the
performance of the best non-parametric model(from the tree-based models)
with the best parametric model from the previous section.

- Data Setup

```{r}
# --- Variable Setup ---
# response variable (target)
response_var <- c("charges")

# Predictor variable set
predictor_vars <- setdiff(
  names(df_ori),
  c(response_var)
)

y_train <- train_ori[,response_var]
y_test <- test_ori[,response_var]

```

#### 3.2.1 Random Forest

Random Forest(RF) is the model based on aggregation(bagging) method
which is robust in reducing variance and preventing overfitting. In our
implementation, we use caret::train() with 200 trees and employ 5-fold
cross-validation(CV) to determine the optimal number of features and
evaluate the model's performance under the best parameter setting.

```{r}

# ==========================================
#  Random Forest model with CV
# ==========================================

set.seed(321)

# --- Parameter Setup ---

# Construct model formula
form_rf <- as.formula(
  paste(response_var, "~", paste(predictor_vars, collapse = "+"))
)

# --- Training model ---

# number of tree (ntree)
B <- 200
# number of predictors
p <- ncol(train_ori) - 1

# rf <- randomForest(form_rf, data = train_raw, 
#                       mtry = p/2, ntree = B, importance = TRUE)

# 5-fold Cross-Validation
cv_ctrl <- trainControl(method = "cv", number = 5)

# --- training random forest with CV ---
rf_cv <- train(
  form_rf, 
  data = train_ori,
  method = "rf",
  trControl = cv_ctrl,
  tuneLength = p-1, # try mtry = 1 to (p-1)
  ntree = B, # B trees
)

# The result of best random forest
rf_cv

# Variable Importance
rf_vimp <- varImp(rf_cv)

rf_vimp

plot(rf_vimp , main = "RF Variable Importance", xlab = "Importance (%)", ylab = "Features")
```

Based on the result, the optimal number of features for the RF model is
5, which achieves the highest $R^2$ and the lowest RMSE. In terms of
variable importance, *smoker*, *bmi*, *children* are the three most
importance variable in our RF model.

```{r}

# ==========================================
#  Computing metrics
# ==========================================

# Predictions
rf_pred_train <- predict(rf_cv, newdata = train_ori)
rf_pred_test <- predict(rf_cv, newdata = test_ori)

# Train RMSE & R2
rf_train_rmse <- computeRMSE(rf_pred_train, y_train)
rf_train_R2 <- computeR2(rf_pred_train, y_train)

# Test RMSE & R2 (the most important metric)
rf_test_rmse <- computeRMSE(rf_pred_test, y_test)
rf_test_R2 <- computeR2(rf_pred_test, y_test)

# Output the result
cat("Training RMSE:", rf_train_rmse, "\n")
cat("Testing RMSE: ", rf_test_rmse, "\n")
cat("Training R2:  ", rf_train_R2, "\n")
cat("Testing R2:   ", rf_test_R2, "\n")


```

With our optimal parameter setting, the RF model achieves a testing
$R^2$ of 0.948, indicating strong predictive ability, and a testing RMSE
of 2822.49, suggesting that on the unseen data the average prediction
error in medical costs is approximately 2,800 USD.

```{r}

# ==========================================
#  Prediction and Residual Plot
# ==========================================

# Build plot DataFrame
plot_data <- data.frame(
  Predicted = rf_pred_test,
  #  Residuals = actual - predicts
  actual = y_test,
  Residuals = y_test - rf_pred_test
)

# --- plots ---
# 1. actual vs predicts (Ideally, the points should follow 45-degree line.)
PvA_rf <- ggplot(plot_data, aes(x = Predicted, y = actual)) +
  geom_point(alpha = 0.5, color = "darkblue") + # scatter point
  geom_abline(color = "red", linetype = "dashed") + # 45-degree
  labs(title = "Prediction vs Actual", 
    x = "Predicted Charges",
    y = "True Charges") +
  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  theme_minimal()

# 2. Residual Distribution (The points should follow horizontal line.)
PvR_rf <- ggplot(plot_data, aes(x = Predicted, y = Residuals)) +
  geom_point(alpha = 0.5, color = "darkblue") + # scatter point
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") + # horizontal
  theme_minimal() +
  scale_x_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  scale_y_continuous(labels = scales::label_number(scale_cut = scales::cut_short_scale()))+
  labs(
    title = "Residual Plot (Random Forest)",
    x = "Predicted Charges",
    y = "Residuals (Actual - Predicted)"
  )

grid.arrange(grobs = list(PvA_rf, PvR_rf), ncol = 2)
```

In the plot of Predictions vs. Actual values, the points generally
follow a 45-degree line, indicating good agreement between predicted and
actual values. In the plot of Predictions vs. Residuals, most points are
distributed near a horizontal line, suggesting that the residuals are
randomly scattered around zero. 
The performance of the RF model is quite good.

#### 3.2.2 XGBoost

XGBoost (XGB) is a gradient boosting model that builds trees
sequentially, with each new tree learning to correct the errors of the
previous ones. This approach allows the model to capture complex
non-linear relationships and subtle interactions between features. In
our analysis, we use cross-validation to determine the optimal number of
boosting rounds and train the final model with the best parameter
settings.

```{r}
# ==========================================
#  XGBoost with CV
# ==========================================

dtrain <- xgb.DMatrix(data = as.matrix(train_ori %>% select(-charges)), label = train_ori$charges)
params <- list(
  booster = "gbtree",
  objective = "reg:tweedie", # Importance! This is the target function for regression
  tweedie_variance_power = 1.1,
  eval_metric = "rmse",       # evaluate model by RMSE
  eta = 0.05,                 # Learning rate
  max_depth = 6,              # max depth of each tree
  subsample = 0.8,            # Row subsampling rate
  colsample_bytree = 1,       # Feature subsampling rate
  gamma = 0.05,                   
  min_child_weight = 5        # Min sum of instance weight needed in a child node
)

# --- Training Model ---

# Using CV to find best rounds (nrounds) and avoid overfitting
cv_model <- xgb.cv(
  set.seed(42),
  params = params,
  data = dtrain,
  nrounds = 1000,        # Upper bound for number of boosting rounds
  nfold = 5,             # 5-fold cross validation
  early_stopping_rounds = 10, # Stop if no improvement for 10 rounds
  verbose = 1,           # Display training progress
  print_every_n = 100
)

# Extract the best number of boosting rounds

best_nrounds <- cv_model$best_iteration


# Train the final XGBoost model with optimal parameters
final_model <- xgb.train(
  set.seed(42),
  params = params,
  data = dtrain,
  nrounds = best_nrounds
)
```

```{r}
dtest <- xgb.DMatrix(data = as.matrix(test_ori %>% select(-charges)), label = test_ori$charges)
pred_xgb <- predict(final_model, dtest)

# --- Accuracy Validation ---
# Compute RMSE
rmse <- computeRMSE(pred_xgb, y_test)

cat("--- Model Evaluation ---\n")
cat("Testing RMSE (USD):", rmse, "\n")

# Compute R-squared
r_sq <- computeR2(pred_xgb, y_test)
cat("Testing R-squared:", r_sq, "\n")
```

With our optimal parameter setting, the XGB model achieves a testing
$R^2$ of 0.963, indicating the outstanding predictive ability, and a
testing RMSE of 2378.16, suggesting that on the unseen data the average
prediction error in medical costs is approximately 2,400 USD.

```{r}
# 1. Compute importance matrix
importance_matrix <- xgb.importance(feature_names = colnames(dtrain), model = final_model)

# 2. View feature importance
print(importance_matrix)

# 3. Draw plot
xgb.plot.importance(importance_matrix)
```

According to the feature importance matrix, we can find *smoker*, *bmi*,
*age* which are also the most three important feature in XGB model.

```{r}
plot_xgb <- data.frame(
  preds = pred_xgb,
  #  Residuals = actual - predicts
  actual = y_test,
  residuals = y_test - pred_xgb
)

# 1. Prediction vs Actual (修正 geom_abline)
PvA_xgb <- ggplot(plot_xgb, aes(x = preds, y = actual)) +
  geom_point(alpha = 0.5, color = "darkblue") +  # 加上顏色區分
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed", size = 1) + # 強制設定斜率為1
  labs(title = "XGBoost: Prediction vs Actual",
       x = "Predicted Charges (USD)",
       y = "Actual Charges (USD)") +
  theme_minimal()

# 2. Residual Plot (修正 geom_hline)
PvR_xgb <- ggplot(plot_xgb, aes(x = preds, y = residuals)) +
  geom_point(alpha = 0.5, color = "darkblue") +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed", size = 1) + # 明確指定水平線
  labs(title = "XGBoost: Residual Analysis",
       x = "Predicted Charges",
       y = "Residuals (Actual - Predicted)") +
  theme_minimal()

PvA_xgb
PvR_xgb
```

```{r}
rho <- 1.1 

xgb_df <- data.frame(
  actuals = test_ori$charges,
  preds = pred_xgb
)

xgb_df$raw_resid <- xgb_df$actuals - xgb_df$preds

xgb_df$pearson_resid <- (xgb_df$actuals - xgb_df$preds) / (xgb_df$preds^(rho / 2))

p1 <- ggplot(xgb_df, aes(x = preds, y = raw_resid)) +
  geom_point(alpha = 0.5, color = "black") +
  geom_hline(yintercept = 0, color = "red") +
  labs(title = "XGBoost Raw Residuals (Original)",
       x = "Predicted Values", y = "y - y_hat") +
  theme_minimal()

p2 <- ggplot(xgb_df, aes(x = preds, y = pearson_resid)) +
  geom_point(alpha = 0.5, color = "blue") +
  geom_hline(yintercept = 0, color = "red") +
  labs(title = "XGBoost Pearson Residuals (Corrected)",
       subtitle = paste0("Standardized by variance power rho = ", rho),
       x = "Predicted Values", y = "Pearson Residuals") +
  theme_minimal()


p1
p2
```

```{r}
# 1. Define a threshold for "severe underestimation" (residual > 10)
# These are cases where the model predicted a low cost, but the actual cost was very high.
outliers <- xgb_df[xgb_df$pearson_resid > 10, ]

# 2. Check the number of these outliers
print(paste("Number of severely underestimated cases:", nrow(outliers)))

# 3. Print and inspect these cases (viewing the first few rows)
head(outliers)

# 4. Compare the average features of "normal cases" vs. "outliers"
summary(xgb_df)           # Statistics for the entire dataset
summary(outliers)         # Statistics for the outliers
```

```{r}
# 定義異常值與正常值
outliers_idx <- which(xgb_df$pearson_resid > 10)
outlier_data <- df_ori[outliers_idx, ]
normal_data <- df_ori[-outliers_idx, ]

# 建立一個比較函數，看比例差異
compare_props <- function(col_name) {
  cat("\n--- Comparison for:", col_name, "---\n")
  
  # 計算 Outlier 組的比例
  prop_outlier <- prop.table(table(outlier_data[[col_name]]))
  cat("Outlier Group Proportions:\n")
  print(round(prop_outlier, 3))
  
  # 計算 Normal 組的比例
  prop_normal <- prop.table(table(normal_data[[col_name]]))
  cat("Normal Group Proportions:\n")
  print(round(prop_normal, 3))
}

# 執行比較 (針對類別變數)
compare_props("smoker")
compare_props("sex")
compare_props("children")

# 針對數值變數 (Age, BMI) 檢查平均數或中位數
cat("\n--- Numeric Comparison ---\n")
cat("Mean BMI - Outliers:", mean(outlier_data$bmi), " vs Normal:", mean(normal_data$bmi), "\n")
cat("Mean Age - Outliers:", mean(outlier_data$age), " vs Normal:", mean(normal_data$age), "\n")
```

```{r}
plot_df <- df_ori 
plot_df$Group <- "Normal"
plot_df$Group[outliers_idx] <- "Outlier"

# 將 Group 轉為因子，確保 Outlier 顯示顏色一致
plot_df$Group <- factor(plot_df$Group, levels = c("Normal", "Outlier"))
```

```{r}
# 定義畫圓餅圖的函數
draw_comparison_pie <- function(data, variable, title_text) {
  
  # 1. 計算百分比
  pie_data <- data %>%
    group_by(Group, !!sym(variable)) %>%
    summarise(count = n(), .groups = 'drop') %>%
    group_by(Group) %>%
    mutate(prop = count / sum(count)) %>%
    mutate(label = sprintf("%1.0f%%", prop * 100)) # 產生百分比標籤
  
  # 2. 繪圖
  # fill = as.factor(!!sym(variable)) 
  # 強制把 0/1 轉成類別，解決 scale_fill_brewer 的報錯
  ggplot(pie_data, aes(x = "", y = prop, fill = as.factor(!!sym(variable)))) +
    geom_bar(width = 1, stat = "identity", color = "white") +
    coord_polar("y", start = 0) +
    facet_wrap(~Group) + 
    geom_text(aes(label = label), 
              position = position_stack(vjust = 0.5), size = 3) +
    # 設定圖例標題為變數名稱，並指定顏色盤
    labs(title = title_text, x = NULL, y = NULL, fill = variable) +
    theme_void() + 
    theme(plot.title = element_text(hjust = 0.5, face = "bold"),
          legend.position = "bottom") +
    scale_fill_brewer(palette = "Pastel1") 
}
plot_df <- plot_df %>%
  mutate(region = case_when(
    regionnorthwest == 1 ~ "northwest",
    regionsoutheast == 1 ~ "southeast",
    regionsouthwest == 1 ~ "southwest",
    TRUE ~ "northeast"  # 如果以上都是 0，那就是基準組 northeast
  ))

# --- 重新執行繪圖 ---

# 1. 比較吸菸者 (Smoker)
p_pie_smoker <- draw_comparison_pie(plot_df, "smokeryes", "Smoker Distribution")

# 2. 比較地區 (Region) -這是新產生的 region 欄位
p_pie_region <- draw_comparison_pie(plot_df, "region", "Region Distribution")

# 3. 顯示圖表
p_pie_smoker
p_pie_region
```

```{r}
# 定義畫箱型圖的函數
draw_comparison_box <- function(data, variable, title_text) {
  ggplot(data, aes(x = Group, y = !!sym(variable), fill = Group)) +
    geom_boxplot(alpha = 0.7, outlier.shape = 21, outlier.fill = "white") +
    labs(title = title_text, x = "", y = variable) +
    theme_minimal() +
    theme(legend.position = "none",
          plot.title = element_text(hjust = 0.5, face = "bold"),
          axis.text.x = element_text(size = 12, face = "bold")) +
    scale_fill_manual(values = c("Normal" = "#3498db", "Outlier" = "#e74c3c"))
}

# --- 產生箱型圖 ---

# 1. 比較 BMI
p_box_bmi <- draw_comparison_box(plot_df, "bmi", "BMI Distribution")

# 2. 比較年齡 (Age)
p_box_age <- draw_comparison_box(plot_df, "age", "Age Distribution")

# 顯示圖表
grid.arrange(p_box_bmi, p_box_age, ncol = 2)
```

##### Conclusion on Xgboost Performance and Error Analysis

The residual analysis confirms that the XGBoost model using the Tweedie
objective successfully captures the general variance of medical costs.
However, Pearson residual diagnostics revealed a specific cluster of
under-predicted outliers ($n=32$).

A detailed inspection shows that these outliers do not possess
conspicuously higher risk factors compared to the general population.
Their average BMI ($\approx 30.5$) is similar to the normal group
($\approx 30.7$), and while they are slightly younger (mean age
$\approx 36$ vs $39$), their demographic profile does not explicitly
justify the extreme costs incurred.

Although the outlier group contains a slightly higher proportion of
smokers (28%) compared to the normal group (20%), the majority are
non-smokers. The model likely under-predicted these cases because their
available features (Average BMI, Younger Age) signaled moderate risk,
yet they incurred high actual expenses. This discrepancy suggests these
costs are driven by aleatoric uncertainty—likely due to random, acute
medical events (e.g., accidents or sudden critical illnesses) that
cannot be predicted by demographic data alone. The model has likely
reached its performance ceiling with the current feature set.

### Model Comparison

```{r}
model_results <- data.frame(
  Model = c("OLS baseline","Lasso (CV λ_min)","Lasso (BIC)","Feature Selection (2 interactions)", "Feature Selection (all interactions)", "Random Forest", "XGBoost"), 
  R2_test = c(R2_test_lm, R2_test_cv_base, R2_test_bic_base, R2_test_fs2, R2_train_2way_lasso, rf_test_R2, r_sq),
  RMSE_test = c(RMSE_test_lm, RMSE_test_cv_base, RMSE_test_bic_base, RMSE_test_fs2, RMSE_test_2way_lasso, rf_test_rmse, rmse)
)

model_results
```


## 4. Application

Based on the predictive capabilities of our best-performing model
(XGBoost with Tweedie objective) and the insights gained from the
exploratory data analysis, we propose several practical applications for
the insurance industry and healthcare management:

### 4.1 Precision Pricing and Dynamic Premium Adjustment

Traditional actuarial tables often rely on broad demographic brackets.
Our model demonstrates that non-linear interactions (e.g., BMI \* Smoker
and Age \* BMI) significantly impact medical costs.

-   Implementation: Insurance companies can deploy this model to
    calculate Individualized Risk Scores. Instead of a flat rate for all
    "smokers," the premium can be dynamically adjusted based on the
    compounding risk of obesity and age.

-   Benefit: This leads to fairer pricing strategies where low-risk
    individuals are not subsidizing high-risk behaviors, potentially
    increasing market competitiveness for the insurer.

### 4.2 Targeted Preventative Health Programs

Our feature importance analysis revealed that lifestyle factors (BMI,
smoking status) are dominant predictors of high costs.

-   Implementation: Insurers can identify high-risk policyholders (e.g.,
    those in the "pre-high-cost" trajectory) and offer subsidized
    intervention programs, such as smoking cessation workshops or gym
    memberships.

-   Benefit: By proactively reducing the risk factors (lowering BMI or
    stopping smoking), insurers can significantly reduce the probability
    of catastrophic claims (e.g., cardiovascular events) in the long
    run, improving the "Loss Ratio."

### 4.3 Fraud Detection and Anomaly Flagging

The error analysis in Section 3.2.2 established a baseline for
"expected" costs.

-   Implementation: The model can serve as a benchmark engine for claims
    processing. If a claim amount significantly deviates from the
    predicted interval (specifically, if Actual Cost \>\> Predicted Cost
    without a clear medical justification like an accident), it can be
    automatically flagged for manual review.

-   Benefit: This assists in identifying potential billing errors,
    over-servicing by providers, or fraudulent claims, thereby reducing
    operational leakage.

### 4.4 Financial Reserving and Solvency

-   Implementation: On a macro level, the model can predict the
    aggregate medical expenditure for the entire portfolio of
    policyholders for the upcoming year.

-   Benefit: This allows the finance department to set aside more
    accurate cash reserves (Incurred But Not Reported - IBNR reserves),
    ensuring the company remains solvent while optimizing capital
    allocation for investments.

## 5. Conclusion

### 5.1 Summary of Findings

In this study, we developed and evaluated a series of statistical
learning models to predict medical insurance costs.

-   Exploratory Analysis: We identified that the distribution of medical
    charges is highly right-skewed and heteroscedastic, which makes prediction 
    more challenging. The main features that affect medical insurance are 
    smoking, BMI, and age. Key interaction effects, particularly between smoking 
    status and BMI, were found to be the strongest drivers of cost escalation.

-   Model Evolution: We progressed from baseline Linear Regression (OLS)
    to regularized methods (Lasso), and finally to non-linear tree-based
    models (Random Forest and XGBoost).

    -   Linear models achieved high $R^2$ but failed to capture the
        complex structure of residuals.
    -   XGBoost utilizing the Tweedie objective function
        ($\rho \approx 1.1$) proved to be the superior model. It
        successfully handled the zero-inflated (compound Poisson-Gamma)
        nature of medical costs, minimizing the RMSE and standardizing
        the Pearson residuals.

### 5.2 Limitations: The Challenge of Aleatoric Uncertainty

Despite the high predictive accuracy ($R^2 \approx 0.96$), our residual
analysis revealed a persistent limitation. A specific cluster of
outliers—young, non-smoking, healthy individuals with skyrocketing
costs—remained difficult to predict.

-   Interpretation: These errors likely represent aleatoric
    uncertainty—random, unpredictable events such as traffic accidents
    or sudden acute injuries that are independent of demographic or
    lifestyle data.

-   Constraint: No amount of parameter tuning or model complexity can
    predict these stochastic events using the current feature set. They
    represent the inherent risk floor of the insurance business.

### 5.3 Future Directions

To further enhance model performance and business utility, we suggest
the following improvements:

1.  Integration of Longitudinal Data: Incorporating historical medical
    claims history (time-series data) rather than a snapshot would allow
    the model to detect chronic disease progression.

2.  External Data Sources: Adding geospatial data (e.g., pollution
    levels in the region) or provider data (hospital pricing tiers)
    could explain regional cost variances.

3.  Two-Stage Modeling: Developing a separate classification model to
    predict the probability of a major accident/hospitalization,
    combined with a regression model for cost severity, might better
    handle the "Black Swan" events identified in our outlier analysis.
